<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai | GerovLabs </title> <meta name="author" content="Sava Gerov"> <meta name="description" content="Going from a manual implementation of gradient descent to SGD through PyTorch and Fast.AI"> <meta name="keywords" content="software development, consulting, freelance, tech, leadership"> <meta property="og:site_name" content="GerovLabs"> <meta property="og:type" content="article"> <meta property="og:title" content="GerovLabs | Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai"> <meta property="og:url" content="https://www.gerovlabs.com/blog/stochastic-gradient-descent-with-pytorch-fastai/"> <meta property="og:description" content="Going from a manual implementation of gradient descent to SGD through PyTorch and Fast.AI"> <meta property="og:image" content="https://gerovlabs.com/assets/img/logo-white-bg.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai"> <meta name="twitter:description" content="Going from a manual implementation of gradient descent to SGD through PyTorch and Fast.AI"> <meta name="twitter:image" content="https://gerovlabs.com/assets/img/logo-white-bg.png"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sava Gerov"
        },
        "url": "https://www.gerovlabs.com/blog/stochastic-gradient-descent-with-pytorch-fastai/",
        "@type": "BlogPosting",
        "description": "Going from a manual implementation of gradient descent to SGD through PyTorch and Fast.AI",
        "headline": "Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai",
        
        "sameAs": ["https://github.com/sgerov", "https://www.linkedin.com/in/savagerov"],
        
        "name": "Sava Gerov",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?2b0a6a1bd6556e942a1fd4e385666f34"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.gerovlabs.com/blog/stochastic-gradient-descent-with-pytorch-fastai/"> <script src="/assets/js/theme.js?7f796469b150645e25b1bc850cbab69e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <img src="/assets/img/logo-white.png" class="repo-img-dark" width="120px"> <img src="/assets/img/logo.png" class="repo-img-light" width="120px"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">learning hub </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai</h1> <p class="post-meta"> Created in November 05, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   ·   <a href="/blog/category/exploration"> <i class="fa-solid fa-tag fa-sm"></i> exploration</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In previous posts we implemented Gradient Descent in Ruby to <a href="/blog/gradient-descent-from-scratch/">approximate a quadratic function</a>, extended it <a href="/blog/estimate-any-function/">to any function</a> with the help of PyTorch tensors and gradients calculation and made a <a href="/blog/from-ball-to-computer-vision/">simple digit classifier</a>.</p> <p>In this post we’ll build on top of those posts and introduce <strong>Stochastic Gradient Descent</strong> while leveraging further PyTorch and Fast.ai libraries to simplify the code we’ve been using so far.</p> <h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3> <p>According to the <a href="https://dictionary.cambridge.org/dictionary/english/stochastic" rel="external nofollow noopener" target="_blank">Cambridge Dictionary</a> definition, <em>a stochastic process or system is connected with random probability</em>. Why would we add randomness in the Gradient Descent we’ve seen so far? It’s all about making the process faster, specially for large datasets.</p> <p>As a reminder, in previous posts we generated a <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> pair and ran all our inputs <code class="language-plaintext highlighter-rouge">x</code> through our neural network to get some predictions <code class="language-plaintext highlighter-rouge">preds</code> which we would later compare to our expected outputs <code class="language-plaintext highlighter-rouge">y</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>In our <a href="/blog/from-ball-to-computer-vision/">last post</a> <code class="language-plaintext highlighter-rouge">model</code> was a linear function at first:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="nd">@weights</span> <span class="o">+</span> <span class="n">bias</span>
</code></pre></div></div> <p>which we later switched to a neural network to improve our predictions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simple_neural_net</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="n">res</span> <span class="o">=</span> <span class="n">x</span><span class="nd">@w1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="nd">@w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></div> <p>We can see that the amount of parameters (weights and biases) and activation functions (sigmoid in this sample) vary depending on what model function we are using in our error optimisation through Gradient Descent. As we increase the hidden layers of a neural network we increase the number of gradients we need to calculate for each input. Additionaly we can increase the dimensions of our input (1 for a quadratic function, 784 for a simple 28x28 grayscale png image).</p> <p>The <a href="/blog/from-ball-to-computer-vision/">simple digit classifier sample</a> (which uses <strong>~12k samples</strong> for its training) with no GPU acceleration already takes <strong>13.8 seconds</strong> to train in my MacBook Pro. With such a small sample my CPU is already struggling!</p> <p>That’s where Stochastic Gradient Descent kicks in: it uses randomised chunks of our <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> input dataset pairs to calculate the gradient on the loss and adjusts our parameters before seeing our whole dataset. E.g. if we split our dataset into 100 chunks, Stochastic Gradient Descent will update the parameters 100 times before it did a full run on our data. That implies that once we did a full epoch (we ran through all data) we already updated our weights and biases 100 times as opposed to only just once <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" rel="external nofollow noopener" target="_blank">while still converging due to the law of large numbers</a>.</p> <h3 id="implementation">Implementation</h3> <p>We can use the PyTorch <em>Dataloders</em> and <em>Datasets</em> to get an iterator of our <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> tensors.</p> <p>A <em>Dataset</em> is just a list of key-value pairs (in its simplest form, there is also a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="external nofollow noopener" target="_blank">Dataset class</a> for more structured and efficient data handling):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">dset</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">dset</span>
<span class="c1"># [(1, 1), (2, 0), (3, 1), (4, 1)]
</span></code></pre></div></div> <p>And a <em>Dataloader</em> gives us an iterator that can go through all data shuffling it randomly in batches of fixed size:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">list</span><span class="p">(</span><span class="n">dl</span><span class="p">)</span>
<span class="c1">#[
#  (tensor([3, 1]), tensor([1, 1])), # random batch 1
#  (tensor([4, 2]), tensor([1, 0]))  # random batch 2
#]
</span></code></pre></div></div> <p>With that, we are ready to go from running through the whole dataset each epoch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fx</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="n">parameters</span> <span class="o">-=</span> <span class="n">parameters</span><span class="p">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
<span class="n">parameters</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
</code></pre></div></div> <p>To doing it in batches, hence adjusting our parameters <code class="language-plaintext highlighter-rouge">data_length / batches</code> times before we do a full epoch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ds</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">dl</span>
  <span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fx</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="n">parameters</span> <span class="o">-=</span> <span class="n">parameters</span><span class="p">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
  <span class="n">parameters</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
</code></pre></div></div> <p>Only applying this change I was able to re-run the <a href="/blog/from-ball-to-computer-vision/">digit classifier sample</a> and reduced the runtime <strong>from 14 to 4 seconds</strong>! A side-effect of running multiple adjustments per epoch is that we will need less epochs to optimise our parameters enough so that we get a good accuracy. In this sample I was able to achieve the same accuracy result (94%) with 2 epochs and a 256 batch size as I did with 100 epochs and no batching at all (with a fixed learning rate).</p> <h3 id="pytorch-and-fastai-constructs">PyTorch and Fast.ai constructs</h3> <p>There are plenty of modules that PyTorch and fast.ai provide to ease the process we’ve described in the last posts.</p> <p>The first simplification we can do is around how we define our linear model:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
-def linear1(x): return x@weights + bias
-weights = init_params((28*28,1))
-bias = init_params(1)
</span><span class="gi">+linear1 = nn.Linear(28*28,1)
+weights, bias = linear1.parameters()
</span></code></pre></div></div> <p>Same principle applies to our neural network:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()
-def simple_net(xb): 
-    res = xb@w1 + b1
-    res = res.sigmoid()
-    res = res@w2 + b2
-    return res
-w1 = init_params((28*28,50))
-b1 = init_params(50)
-w2 = init_params((50,1))
-b2 = init_params(1)
</span><span class="gi">+simple_net = nn.Sequential(
+    nn.Linear(28*28,50),
+    nn.Sigmoid(),
+    nn.Linear(50,1)
+)
+w1, b1, w2, b2 = simple_net.parameters()
</span></code></pre></div></div> <p>Another useful abstraction we can use is in the optimisation step where fastai provides SGD class to handle it:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-with torch.no_grad(): parameters -= parameters.grad * lr
-parameters.grad = None
</span><span class="gi">+opt = SGD(simple_net.parameters(), lr)
+opt.step()
+opt.zero_grad()
</span></code></pre></div></div> <p>We can go one step further with fast.ai and actually user a learner for the training process with no custom training code at all:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-for xb, yb in dl
-  preds = simple_net(xb)
-  loss = loss_fx(preds, yb)
-  loss.backward()
-  with torch.no_grad(): parameters -= parameters.grad * lr
-  parameters.grad.zero_()
</span><span class="gi">+dls = DataLoaders(dl, ()) # skipping the validation set and accuracy
+learn = Learner(dls, simple_net, opt_func=SGD,
+                loss_func=mnist_loss)
+learn.fit(20, lr=0.01)
</span></code></pre></div></div> <h3 id="full-example">Full example</h3> <p>The entire <a href="/blog/from-ball-to-computer-vision/">digit classifier example</a> would turn into the following (non-refactored for the sake of explicitness) code:</p> <p><strong>Import dependencies</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
</code></pre></div></div> <p><strong>Data setup</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="nf">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="p">.</span><span class="n">MNIST</span><span class="p">)</span>

<span class="c1"># fives
</span><span class="n">fives_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">fives_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">fives_filenames</span><span class="p">]</span>
<span class="n">fives</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">fives_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">fours_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">4</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">fours_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">fours_filenames</span><span class="p">]</span>
<span class="n">fours</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">fours_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>

<span class="c1"># fours
</span><span class="n">validation_fives_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">testing</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">validation_fives_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">validation_fives_filenames</span><span class="p">]</span>
<span class="n">validation_fives</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">validation_fives_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">validation_fours_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">testing</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">4</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">validation_fours_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">validation_fours_filenames</span><span class="p">]</span>
<span class="n">validation_fours</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">validation_fours_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>

<span class="c1"># x and y for training and validation
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">fives</span><span class="p">,</span> <span class="n">fours</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">fives</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">fours</span><span class="p">)).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">validation_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">validation_fives</span><span class="p">,</span> <span class="n">validation_fours</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">validation_y</span> <span class="o">=</span> <span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">validation_fives</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">validation_fours</span><span class="p">)).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># datasets and dataloaders
</span><span class="n">dset</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">))</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">valid_dset</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">valid_x</span><span class="p">,</span><span class="n">valid_y</span><span class="p">))</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">valid_dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="nc">DataLoaders</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Functions used during training</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mnist_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">batch_accuracy</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">preds</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">==</span> <span class="n">yb</span>
    <span class="k">return</span> <span class="n">correct</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Training with simple neural net</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">simple_net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="nc">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">simple_net</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">SGD</span><span class="p">,</span>
                <span class="n">loss_func</span><span class="o">=</span><span class="n">mnist_loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">batch_accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre></div></div> <h3 id="leveraging-fastai-further">Leveraging fast.ai further</h3> <p>We could go a few steps further and leverage a <code class="language-plaintext highlighter-rouge">vision_learner</code> from fast.ai with its <code class="language-plaintext highlighter-rouge">ImageDataLoaders</code> that handle all the data loading (for all numbers), training, accuracy calculation and fitting for us within only 3 lines of code (and a 18-layers architecture):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="p">.</span><span class="nf">from_folder</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="sh">'</span><span class="s">testing</span><span class="sh">'</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="nf">vision_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                    <span class="n">loss_func</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="nf">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span> <span class="c1"># fit with learning rate scheduling for quicker convergence
</span></code></pre></div></div> <p>I’ve deployed another sample leveraging fast.ai abstractions further to finetune a resnet18 pretrained model to <a href="/blog/fastai-finetuning-a-classifier/">recognise LOTR characters from photos</a> with <a href="https://gerovlabs.com/ai-models/" rel="external nofollow noopener" target="_blank">a working custom deployment</a>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/from-ball-to-computer-vision/">From approximating math functions to computer vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/estimate-any-function/">Estimate any function with Gradient Descent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/gradient-descent-from-scratch/">Gradient Descent from scratch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-bias-and-feedback-loops/">AI Bias and Feedback loops</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fastai-finetuning-a-classifier/">Deploying a fine-tuned classifier</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="newsletter-form-container"> <form class="newsletter-form" action="https://app.loops.so/api/newsletter-form/cm0teszzw00txtql65tlwc8r9" method="POST" style="justify-content: center"> <input class="newsletter-form-input" name="newsletter-form-input" type="email" placeholder="user@example.com" required=""> <button type="submit" class="newsletter-form-button" style="justify-content: center"> subscribe </button> <button type="button" class="newsletter-loading-button" style="justify-content: center"> Please wait... </button> </form> <div class="newsletter-success" style="justify-content: center"> <p class="newsletter-success-message">You're subscribed!</p> </div> <div class="newsletter-error" style="justify-content: center"> <p class="newsletter-error-message">Oops! Something went wrong, please try again</p> </div> <button class="newsletter-back-button" type="button" onmouseout='this.style.textDecoration="none"' onmouseover='this.style.textDecoration="underline"'> ← Back </button> </div> <script>function submitHandler(e){e.preventDefault();var t=e.target.parentNode,r=t.querySelector(".newsletter-form"),s=t.querySelector(".newsletter-form-input"),l=t.querySelector(".newsletter-success"),n=t.querySelector(".newsletter-error"),o=t.querySelector(".newsletter-error-message"),a=t.querySelector(".newsletter-back-button"),i=t.querySelector(".newsletter-form-button"),y=t.querySelector(".newsletter-loading-button");const d=()=>{n.style.display="flex",o.innerText="Too many signups, please try again in a little while",i.style.display="none",s.style.display="none",a.style.display="block"};var m=(new Date).valueOf(),c=localStorage.getItem("loops-form-timestamp");if(c&&Number(c)+6e4>m)d();else{localStorage.setItem("loops-form-timestamp",m),i.style.display="none",y.style.display="flex";var u="userGroup=&email="+encodeURIComponent(s.value);fetch(e.target.action,{method:"POST",body:u,headers:{"Content-Type":"application/x-www-form-urlencoded"}}).then(e=>[e.ok,e.json(),e]).then(([e,t,s])=>{e?(l.style.display="flex",r.reset()):t.then(e=>{n.style.display="flex",o.innerText=e.message?e.message:s.statusText})})["catch"](e=>{"Failed to fetch"!==e.message?(n.style.display="flex",e.message&&(o.innerText=e.message),localStorage.setItem("loops-form-timestamp","")):d()})["finally"](()=>{s.style.display="none",y.style.display="none",a.style.display="block"})}}function resetFormHandler(e){var t=e.target.parentNode,r=t.querySelector(".newsletter-form-input"),s=t.querySelector(".newsletter-success"),l=t.querySelector(".newsletter-error"),n=t.querySelector(".newsletter-error-message"),o=t.querySelector(".newsletter-back-button"),a=t.querySelector(".newsletter-form-button");s.style.display="none",l.style.display="none",n.innerText="Oops! Something went wrong, please try again",o.style.display="none",r.style.display="flex",a.style.display="flex"}for(var formContainers=document.getElementsByClassName("newsletter-form-container"),i=0;i<formContainers.length;i++){var formContainer=formContainers[i],handlersAdded=formContainer.classList.contains("newsletter-handlers-added");handlersAdded||(formContainer.querySelector(".newsletter-form").addEventListener("submit",submitHandler),formContainer.querySelector(".newsletter-back-button").addEventListener("click",resetFormHandler),formContainer.classList.add("newsletter-handlers-added"))}</script> <noscript> <style>.newsletter-form-container{display:none}</style> </noscript> <div class="container"> © Copyright 2024 Gerovlabs ltd - sava [AT] gerov.es </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="/assets/js/typograms.js?63f3caa50c7a9624f953b3aec207afa6"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-typograms").forEach(e=>{const t=e.textContent,n=e.parentElement.parentElement;let a=document.createElement("pre");a.classList.add("typogram");const d=create("\n"+t,.3,!1);a.appendChild(d),n.appendChild(a),n.removeChild(e.parentElement)})});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-LW56FFFC4E"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-LW56FFFC4E");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-learning-hub",title:"learning hub",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-stochastic-gradient-descent-sgd-with-pytorch-and-fast-ai",title:"Stochastic Gradient Descent (SGD) with PyTorch and Fast.ai",description:"Going from a manual implementation of gradient descent to SGD through PyTorch and Fast.AI",section:"Posts",handler:()=>{window.location.href="/blog/stochastic-gradient-descent-with-pytorch-fastai/"}},{id:"post-from-approximating-math-functions-to-computer-vision",title:"From approximating math functions to computer vision",description:"Let&#39;s look at how neural networks allow to solve predictions beyond quadratic functions.",section:"Posts",handler:()=>{window.location.href="/blog/from-ball-to-computer-vision/"}},{id:"post-estimate-any-function-with-gradient-descent",title:"Estimate any function with Gradient Descent",description:"Let&#39;s have a look how we can resolve more generic functions with Gradient Descent by combining linear functions and non-linear ones.",section:"Posts",handler:()=>{window.location.href="/blog/estimate-any-function/"}},{id:"post-gradient-descent-from-scratch",title:"Gradient Descent from scratch",description:"A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample.",section:"Posts",handler:()=>{window.location.href="/blog/gradient-descent-from-scratch/"}},{id:"post-ai-bias-and-feedback-loops",title:"AI Bias and Feedback loops",description:"On how AI imposes a danger and further perpetuates human flaws.",section:"Posts",handler:()=>{window.location.href="/blog/ai-bias-and-feedback-loops/"}},{id:"post-deploying-a-fine-tuned-classifier",title:"Deploying a fine-tuned classifier",description:"Leveraging fast.ai capabilities for a quick resnet model fine-tuning and delivery.",section:"Posts",handler:()=>{window.location.href="/blog/fastai-finetuning-a-classifier/"}},{id:"post-simplicity-and-pragmatism",title:"Simplicity and pragmatism",description:"The power of doing less.",section:"Posts",handler:()=>{window.location.href="/blog/simplicity-and-pragmatism/"}},{id:"post-integrity-and-transparency",title:"Integrity and transparency",description:"By being upfront about risks, challenges, and solutions we earn the trust of clients, stakeholders, and team.",section:"Posts",handler:()=>{window.location.href="/blog/integrity-and-transparency/"}},{id:"post-ethical-responsibility",title:"Ethical responsibility",description:"The systems we design and the software we create shape industries, impact individuals, and define modern society.",section:"Posts",handler:()=>{window.location.href="/blog/ethical-responsibility/"}},{id:"post-delivering-value-over-process",title:"Delivering value over process",description:"The real goal of any software project should be rather simple - delivering value.",section:"Posts",handler:()=>{window.location.href="/blog/deliver-value/"}},{id:"post-collaboration-and-communication",title:"Collaboration and communication",description:"Managing expectations and ensuring alignment across and within teams.",section:"Posts",handler:()=>{window.location.href="/blog/collaboration-and-communication/"}},{id:"post-continuous-learning-and-adaptability",title:"Continuous learning and adaptability",description:"In software engineering, things change fast and often.",section:"Posts",handler:()=>{window.location.href="/blog/continuous-learning/"}},{id:"post-technical-excellence",title:"Technical excellence",description:"Building high-quality systems that last.",section:"Posts",handler:()=>{window.location.href="/blog/technical-excellence/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>