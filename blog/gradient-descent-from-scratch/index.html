<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Descent from scratch | GerovLabs </title> <meta name="author" content="Sava Gerov"> <meta name="description" content="A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample."> <meta name="keywords" content="software development, consulting, freelance, tech, leadership"> <meta property="og:site_name" content="GerovLabs"> <meta property="og:type" content="article"> <meta property="og:title" content="GerovLabs | Gradient Descent from scratch"> <meta property="og:url" content="https://www.gerovlabs.com/blog/gradient-descent-from-scratch/"> <meta property="og:description" content="A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample."> <meta property="og:image" content="https://gerovlabs.com/assets/img/logo-white-bg.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Gradient Descent from scratch"> <meta name="twitter:description" content="A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample."> <meta name="twitter:image" content="https://gerovlabs.com/assets/img/logo-white-bg.png"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sava Gerov"
        },
        "url": "https://www.gerovlabs.com/blog/gradient-descent-from-scratch/",
        "@type": "BlogPosting",
        "description": "A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample.",
        "headline": "Gradient Descent from scratch",
        
        "sameAs": ["https://github.com/sgerov", "https://www.linkedin.com/in/savagerov"],
        
        "name": "Sava Gerov",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?2b0a6a1bd6556e942a1fd4e385666f34"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.gerovlabs.com/blog/gradient-descent-from-scratch/"> <script src="/assets/js/theme.js?7f796469b150645e25b1bc850cbab69e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <img src="/assets/img/logo-white.png" class="repo-img-dark" width="120px"> <img src="/assets/img/logo.png" class="repo-img-light" width="120px"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">learning hub </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient Descent from scratch</h1> <p class="post-meta"> Created in October 16, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   ·   <a href="/blog/category/exploration"> <i class="fa-solid fa-tag fa-sm"></i> exploration</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The following article describes how Gradient Descent works which lies at the core of Neural Networks. Don’t worry like I did, only basic Algebra and Calculus are needed to understand well the underlying math and I’ll go through it in detail.</p> <h2 id="problem-statement">Problem statement</h2> <p>Let’s start defining what are we trying to achieve. Given an input, we want to find a function <code class="language-plaintext highlighter-rouge">f(x)</code> that would predict an output with enough accuracy for it to be useful for us as humans.</p> <pre><code class="language-typograms"> input +--------+ output
------&gt;|  f(x)  |-------&gt;
   x   +--------+   y
</code></pre> <p>Let’s look at some samples of some functions we might be interested in:</p> <pre><code class="language-typograms">       time       +--------+  height of ball
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

      picture     +--------+ identify number
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

    audio song    +--------+      genre
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

       text       +--------+      topic
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+
</code></pre> <p>The outputs depend on the inputs and we can find a mathematical function that relates the two. Finding what that function is would allow us to get an output prediction for each input as long as the function resembles the real world closely enough!</p> <p>And since math functions do represent real world phenomenons, let’s just use one such example. The following quadratic function tells us the height of a thrown ball upwards for a particular time <code class="language-plaintext highlighter-rouge">t</code>:</p> \[h(t)=−\frac 1 2 gt^2+v0​t+h0​\] <p>where:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">g</code> is gravity on earth level</li> <li> <code class="language-plaintext highlighter-rouge">v0</code> is the initial throwing speed</li> <li> <code class="language-plaintext highlighter-rouge">h0</code> is the initial height of the throw</li> </ul> <p>If we wanted to predict the height at a particular time (e.g. after 2 seconds) we just need to know the initial throwing speed <code class="language-plaintext highlighter-rouge">v0</code> and height <code class="language-plaintext highlighter-rouge">h0</code> and we can easily resolve <code class="language-plaintext highlighter-rouge">h(2)</code> and get our answer.</p> <p>What if we don’t know the actual formula nor initial throwing/height speed but only have observations from nature? Let’s say a ball was thrown vertically and we installed a sensor which gave us 15 measurements of all the heights the ball went through during 5 seconds:</p> <pre><code class="language-chartjs">{
  "type": "scatter",
  "data": {
    "labels": [0.0, 0.35714285714285715, 0.7142857142857143, 1.0714285714285714, 1.4285714285714286, 1.7857142857142858, 2.142857142857143, 2.5, 2.857142857142857, 3.2142857142857144, 3.5714285714285716, 3.928571428571429, 4.285714285714286, 4.642857142857143, 5.0],
    "datasets": [
      {
        "label": "Height of thrown ball",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [29.128620665942556, 35.24960579584343, 42.16851569710115, 44.90160627870316, 46.63155173360897, 52.22662725576406, 52.86940417603709, 50.56015123210324, 46.128434083285384, 44.53718441643712, 40.57409445955629, 32.19438620635332, 24.50774696452207, 17.09045405656899, 7.26565053601106],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x = time in seconds"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y = height in meters"
        }
      }
    }
  }
}
</code></pre> <p>In this example we want to be able to predict where the ball will be at a certain point in time without knowing the <code class="language-plaintext highlighter-rouge">h(t)</code> formula. For now let’s assume the phenomenon follows some quadratic function:</p> \[f(x)=ax^2 + bx + c\] <p>So we are looking for coefficients <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> that will produce values as similar as possible to the input data we have provided through our sensors.</p> <p>For example, if <code class="language-plaintext highlighter-rouge">a = 3</code>, <code class="language-plaintext highlighter-rouge">b = 2</code>, <code class="language-plaintext highlighter-rouge">c = 1</code> we would have \(f(x)=3x^2 + 2x + 1\). Take one of our real inputs (e.g. <code class="language-plaintext highlighter-rouge">x = 0.714</code>), and we see that \(f(0.714) = 3.957388\). From our input data we know that <code class="language-plaintext highlighter-rouge">y = 42.169</code> for that value of <code class="language-plaintext highlighter-rouge">x</code> so the difference (or error) of our prediction is \(y - f(x)\) which is <code class="language-plaintext highlighter-rouge">42.169 - 3.957388 = 38.211612</code>. We are still far from having a function that predicts well our ball trajectory!</p> <p>Our goal is to minimise the error between our input data and a quadratic function we are looking for (by choosing <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> coefficients).</p> <p>How can we measure the error of the whole function and not for a single point only?</p> <h2 id="calculating-errors">Calculating errors</h2> <p>For a single point we relied on their distance \(y - f(x)\). We want to know the error across all input data we have so we can properly evaluate if our function is good enough. We can achieve that by just taking the mean of all individual errors! For convenience, instead of using <em>function notation</em> we can use our dependant variable name (like we do with <code class="language-plaintext highlighter-rouge">y</code>), i.e. \(\hat y = f(x)\). Our mean would look like this:</p> \[\frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)\] <p>where <code class="language-plaintext highlighter-rouge">n</code> goes from the first sensor datapoint to the last (0 to 15 in our example). The issue with this function is that our predictions might also be negative (if \(\hat y_i &gt; y_i\)) which would screw the mean. We need a more resilient function and a common way to bypass that problem is to use the square of a number since a squared negative result would become positive:</p> \[MSE = \frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)^2\] <p>That is the <strong>Mean Square Error (MSE)</strong> function. It has a problem though, squaring each number implies that our error will be way higher than the actual <code class="language-plaintext highlighter-rouge">y</code> values of our function. In our example, our error was <code class="language-plaintext highlighter-rouge">38.211612</code> which when squared already goes up to <code class="language-plaintext highlighter-rouge">1460.12729164</code>. If we aren’t willing to handle such big numbers in our errors we can take the square root on top of our error which would reduce our number again:</p> \[RMSE = \sqrt {\frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)^2}\] <p>That is the <strong>Root Mean Square Error (RMSE)</strong> function.</p> <p>One of the main issues of both functions is the squaring process which has sensitivity to outliers (bigger numbers will be way bigger, numbers close to 0 will be way smaller). To avoid the issue, we could get the absolute value of the difference instead:</p> \[MAE = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>That is the <strong>Mean Absolute Error (MAE)</strong> function and we will be using it to calculate how far are we with our predictions from the sensor values. One of its major drawbacks is that it’s not differentiable and we’ll explain shortly why is that important.</p> <p>Now that both our problem statement and measure of success are clear we need a resolution method!</p> <h2 id="solution">Solution</h2> <p>In the sample above we chose random values for <code class="language-plaintext highlighter-rouge">a = 3</code>, <code class="language-plaintext highlighter-rouge">b = 2</code> and <code class="language-plaintext highlighter-rouge">c = 1</code> which results in an error of <code class="language-plaintext highlighter-rouge">MAE=32.98</code>. The lower the error, the better we are predicting our input data. How can we adjust <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> iteratively while minimising the error?</p> <p>One rather tedious approach would be to adjust the coeficients randomly and watch how <code class="language-plaintext highlighter-rouge">MAE</code> changes with each coefficient adjustment (for the better or the worst) until we achieve to reduce the <code class="language-plaintext highlighter-rouge">MAE</code> enough. Luckily there is a much better approach through differentiation.</p> <h3 id="differentiation">Differentiation</h3> <p>The question that has just arised is how much would a function change by a change on one of its parameters. In particular, we are asking how much would <code class="language-plaintext highlighter-rouge">MAE</code> output change by a slight change on one of its parameters (e.g. <code class="language-plaintext highlighter-rouge">a</code>).</p> <p>Let’s start with a simpler example, a line \(f(x) = x\):</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [0,1, 2],
    "datasets": [
      {
        "label": "x",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [0, 1, 2],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>By looking at the plot it’s easy to conclude that for each unit increment of <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">y</code> would increment by 1. This is also known as the <em>slope</em> of the tangent line at that point (or <em>rate of change</em>) and it’s described by \(slope = \frac {rise} {run}\), i.e. \(slope = \frac {y_2 - y_1} {x_2 - x_1}\).</p> <p>Let’s now have a look at a more complicated example, a non-linear function like \(f(x) = x^2\):</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [-4, -3, -2, -1, 0, 1, 2, 3, 4],
    "datasets": [
      {
        "label": "x^2",
        "fill": false,
        "lineTension": 0.3,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "bevel",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 0,
        "pointRadius": 0,
        "pointHitRadius": 10,
        "data": [16, 9, 4, 1, 0, 1, 4, 9, 16],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>Here it becomes slightly trickier to know by how much would <code class="language-plaintext highlighter-rouge">y</code> change if <code class="language-plaintext highlighter-rouge">x</code> changed by one because depending on where you look you would get a different slope due to the lack of linearity in our function. For example from <code class="language-plaintext highlighter-rouge">x=4</code> to <code class="language-plaintext highlighter-rouge">x=3</code> it’s \(\frac {16 - 9} {4 - 3} = 7\) but from <code class="language-plaintext highlighter-rouge">x=3</code> to <code class="language-plaintext highlighter-rouge">x=2</code> we get \(\frac {9 - 4} {3 - 2} = 5\).</p> <p>Here is where differentiation kicks in! If we zoom in the slope for <code class="language-plaintext highlighter-rouge">x</code> further then <code class="language-plaintext highlighter-rouge">3</code> to <code class="language-plaintext highlighter-rouge">2</code> and look at values closer to <code class="language-plaintext highlighter-rouge">2</code> we would be getting closer to the real slope of the tangent line in that point (<code class="language-plaintext highlighter-rouge">x=2</code>):</p> <ul> <li> <code class="language-plaintext highlighter-rouge">3.000</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {9 - 4} {3 - 2} = 5\)</li> <li> <code class="language-plaintext highlighter-rouge">2.500</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {6.25 - 4} {2.5 - 2} = 4.5\)</li> <li> <code class="language-plaintext highlighter-rouge">2.250</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {5.0625 - 4} {2.25 - 2} = 4.25\)</li> <li> <code class="language-plaintext highlighter-rouge">2.010</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {4.0401 - 4} {2.01 - 2} = 4.01\)</li> <li> <code class="language-plaintext highlighter-rouge">2.001</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {4.004001 - 4} {2.001 - 2} = 4.001\)</li> </ul> <p>Looks like we are approaching the slope of <code class="language-plaintext highlighter-rouge">4</code>! We could try the same process from <code class="language-plaintext highlighter-rouge">1</code> to <code class="language-plaintext highlighter-rouge">2</code> and we would be approaching the same number. Since <code class="language-plaintext highlighter-rouge">y = f(x)</code> we could express the difference (or <em>slope</em> of tangent line) of any the two points like this:</p> \[\frac {f(x_2) - f(x_1)} {x_2 - x_1}\] <p>As we have seen above we can pick any \(x_2\) that is close enough to \(x_1\) to get our slope so we can formalise that with any constant \(x_2 = x_1 + h\) hence getting:</p> \[\frac {f(x_1 + h) - f(x_1)} {x_1 + h - x_1} = \frac {f(x + h) - f(x)} { h }\] <p>If we want to get a point very close to <code class="language-plaintext highlighter-rouge">x + h</code> we can look at the limit of <code class="language-plaintext highlighter-rouge">h</code> towards <code class="language-plaintext highlighter-rouge">0</code>:</p> \[\lim _{h\to 0} \frac {f(x + h) - f(x)} { h } = \lim _{h\to 0} \frac {\Delta x} { \Delta h }\] <p>That approximation to a point is what differentiation is all about! Let’s try to resolve that limit for \(x^2\):</p> <p>\(\lim _{h\to 0} \frac {f(x + h) - f(x)} { h } =\) \(\lim _{h\to 0} \frac {(x + h)^2 - x^2} { h } =\) \(\lim _{h\to 0} \frac {x^2 + h^2 + 2xh - x^2} { h } =\) \(\lim _{h\to 0} \frac {h^2 + 2xh} { h } =\) \(\lim _{h\to 0} \frac {h(h+2x)} { h } =\) \(\lim _{h\to 0} h+2x = 2x\)</p> <p>We just derived \(x^2\)! If you recall <a href="https://en.wikipedia.org/wiki/Power_rule" rel="external nofollow noopener" target="_blank">the power rule</a> it tells us that given \(f(x) = x^n\) it’s derivative is:</p> <ul> <li>Prime notation: \(f'(x) = nx^{n-1}\)</li> <li>Leibniz notation: \(\frac d {dx} f(x) = nx^{n-1}\)</li> <li>Euler notation: \(\frac \partial {\partial x} f(x) = nx^{n-1}\)</li> </ul> <p><a href="https://en.wikipedia.org/wiki/Differentiation_rules" rel="external nofollow noopener" target="_blank">There are more rules</a> for differentiation which will allow us to get the <em>slope</em> (or <em>rate of change</em>) for any differentiable function (continuous, smooth and without tangents).</p> <p>Actually the slope we got earlier by approximating <code class="language-plaintext highlighter-rouge">x=2</code> was <code class="language-plaintext highlighter-rouge">4</code>, i.e. \(slope = f'(x^2) = 2x = 2*2 = 4\).</p> <h3 id="impact-of-changing-each-parameter">Impact of changing each parameter</h3> <p>At this point, we have defined our problem statement, we are able to measure its’ error for certain parameters and we are able to see how much a change in each parameter impacts that error by using the derivatives of each parameter (or <em>gradient</em>)! As a reminder, the function we are trying to minimise as described above is the <code class="language-plaintext highlighter-rouge">MAE</code>:</p> \[MAE = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>We will start by looking at what’s the impact of each change in <code class="language-plaintext highlighter-rouge">a</code> to that function (so that we can find an <code class="language-plaintext highlighter-rouge">a</code> that minimises its value) by differentiation. Our function is composite, since <code class="language-plaintext highlighter-rouge">MAE</code> depends on the output of our quadratic function. We need to apply <a href="https://en.wikipedia.org/wiki/Chain_rule" rel="external nofollow noopener" target="_blank">the Chain Rule</a> to be able to take the derivative, which states:</p> \[h'(x)=f'(g(x))g'(x)\] <p>Applied to our case, the inner function <code class="language-plaintext highlighter-rouge">g(x)</code> is:</p> \[g(x) = \hat y_i = ax_i^2 + bx_i + c\] <p>and the outer function <code class="language-plaintext highlighter-rouge">f(x)</code> is:</p> \[f(x) = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>Resolving the derivative of our <code class="language-plaintext highlighter-rouge">MAE</code> to find out the impact of <code class="language-plaintext highlighter-rouge">a</code> through the chain rule, we get:</p> <p>\(\frac \partial {\partial a} MAE = \frac \partial {\partial a} \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i| =\) \(\frac \partial {\partial {\hat y_i}} \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i| * { \frac \partial {\partial a} \hat y_i } =\) \(\frac 1 n \sum _{i=1} ^n \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| * { \frac \partial {\partial a} \hat y_i }\)</p> <p>Now we have two derivatives to resolve, let’s do that one by one:</p> <ol> <li> \[{ \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| }\] </li> <li> \[{ \frac \partial {\partial a} \hat y_i }\] </li> </ol> <p>Earlier we mentioned that one problem with <code class="language-plaintext highlighter-rouge">MAE</code> is that it’s not differentiable, and that’s because of the shape of the absolute function <code class="language-plaintext highlighter-rouge">|x|</code>:</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [-1,0, 1],
    "datasets": [
      {
        "label": "x",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [1, 0, 1],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>Whenever <code class="language-plaintext highlighter-rouge">x=0</code> (\(\hat y_i - y_i = 0\)), we don’t know what’s the slope of the line! Looking from the left its negative and looking from the right it’s positive. We need to rely on a <a href="https://en.wikipedia.org/wiki/Subderivative" rel="external nofollow noopener" target="_blank">subdifferentials</a> for this function. For <code class="language-plaintext highlighter-rouge">x=0</code> it gives us a range of <code class="language-plaintext highlighter-rouge">[-1, 1]</code>. By convention we will pick the number <code class="language-plaintext highlighter-rouge">y=0</code> when <code class="language-plaintext highlighter-rouge">x=0</code>. The other particularities of the absolute function is that it converts all numbers to positive, i.e. <code class="language-plaintext highlighter-rouge">if x&gt;0 y=x</code> and <code class="language-plaintext highlighter-rouge">if x&lt;0 y=-x</code>.</p> <p>Now let’s apply the derivative knowing that:</p> <p>\({ \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| } = {\begin{cases} \frac \partial {\partial {\hat y_i}} (-y_i + \hat y_i) &amp; {y_i - \hat y_i} &lt; 0 \\ \frac \partial {\partial {\hat y_i}} (y_i - \hat y_i) &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}} =\) \({\begin{cases} 1 &amp; {y_i - \hat y_i} &lt; 0 \\ -1 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>The other derivative is just a simple application of the Power Rule:</p> <p>\(\frac \partial {\partial a} \hat y_i =\) \(\frac \partial {\partial a} ax^2 + bx + c = x^2\)</p> <p>And we got our <code class="language-plaintext highlighter-rouge">Quadratic MAE</code> full derivative with respect to <code class="language-plaintext highlighter-rouge">a</code>:</p> <p>\(\frac \partial {\partial a} MAE =\) \({\begin{cases} x^2 &amp; {y_i - \hat y_i} &lt; 0 \\ -x^2 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>Repeating the exact same process for <code class="language-plaintext highlighter-rouge">b</code> the only change is the <a href="https://en.wikipedia.org/wiki/Partial_derivative" rel="external nofollow noopener" target="_blank">partial derivative</a> of our quadratic:</p> <p>\(\frac \partial {\partial b} MAE =\) \({\begin{cases} x &amp; {y_i - \hat y_i} &lt; 0 \\ -x &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>and the same for <code class="language-plaintext highlighter-rouge">c</code>:</p> <p>\(\frac \partial {\partial c} MAE =\) \({\begin{cases} 1 &amp; {y_i - \hat y_i} &lt; 0 \\ -1 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>Now we have a way to calculate how much of an impact a unit change on <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> or <code class="language-plaintext highlighter-rouge">c</code> has on our error function with our quadratic. That vector is our <em>gradient</em>.</p> <h3 id="gradient-descent">Gradient descent</h3> <p>We are all set to find a solution to our original problem statement through Gradient Descent. There is <a href="https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent" rel="external nofollow noopener" target="_blank">a nice analogy in Wikipedia</a> to help grasp Gradient Descent: a person is in a mountain and wants to go down. There is plenty of fog so the easiest way down is by looking at the slope and going one step at a time downwards all the way until the slope is 0.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient-descent-480.webp 480w,/assets/img/gradient-descent-800.webp 800w,/assets/img/gradient-descent-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/gradient-descent.jpg" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The slope in our case is the direction to minimise our error and gradient descent would look like:</p> <ul> <li>Start with a random values for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> coefficients (step 0)</li> <li>Get the slopes for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> </li> <li>Update <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> in the opposite direction of the gradient (since we want to decrease the error) by applying a learning rate to ensure we move in small steps (and not whole units)</li> <li>Repeat until we reduced the error rate enough or reach a slope of 0</li> </ul> <h3 id="a-sample-in-ruby">A sample in Ruby</h3> <p>Let’s see how the resolution of our original problem would look like in Ruby. First we need the slopes for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> given \(x_i\), \(y_i\) and \(\hat y_i\):</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_mae_gradient_for_abc</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">length</span>

  <span class="n">quadratic_partials</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span> <span class="o">|</span><span class="n">partial_derivative</span><span class="o">|</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">).</span><span class="nf">map</span> <span class="k">do</span> <span class="o">|</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="o">|</span>
      <span class="n">abs_subgradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">partial_derivative</span><span class="p">.</span><span class="nf">call</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">end</span>

    <span class="n">gradients</span><span class="p">.</span><span class="nf">sum</span> <span class="o">/</span> <span class="n">n</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">abs_subgradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span> <span class="o">&lt;</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span> <span class="o">&gt;</span> <span class="mi">0</span>
  
  <span class="mi">0</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">quadratic_partials</span>
  <span class="n">quadratic_partial_a</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="p">}</span>
  <span class="n">quadratic_partial_b</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">x</span> <span class="p">}</span>
  <span class="n">quadratic_partial_c</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">_</span> <span class="p">{</span> <span class="mi">1</span> <span class="p">}</span>
  
  <span class="p">[</span><span class="n">quadratic_partial_a</span><span class="p">,</span> <span class="n">quadratic_partial_b</span><span class="p">,</span> <span class="n">quadratic_partial_c</span><span class="p">]</span>
<span class="k">end</span>
</code></pre></div></div> <p>Now that we can calculate the gradients we can go down the slope:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_gradient_descent</span><span class="p">(</span><span class="n">initial_parameters</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">)</span>
  <span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">initial_parameters</span>

  <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">..</span><span class="n">iterations</span> <span class="k">do</span>
    <span class="nb">print</span> <span class="s2">"</span><span class="se">\r</span><span class="s2">Running iterations.. (</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="s2">) with learning rate </span><span class="si">#{</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">predicted_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">predicted_parameters</span><span class="p">)</span>
    <span class="n">predicted_y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">predicted_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">calculate_mae_gradient_for_abc</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">)</span>

    <span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">predicted_parameters</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">).</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">param</span><span class="p">,</span> <span class="n">slope</span><span class="o">|</span> <span class="n">param</span> <span class="o">-</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="p">}</span>
  <span class="k">end</span>

  <span class="n">predicted_parameters</span>
<span class="k">end</span>
</code></pre></div></div> <p>We use a small helper to create generic quadratic functions so that we can evaluate the predictions:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quadratic_from_parameters</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
  <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span> <span class="p">}</span>
<span class="k">end</span>
</code></pre></div></div> <p>Now we can execute gradient descent on our problem! Let’s first see how we generate some fake sensor data, starting with our independent variable <code class="language-plaintext highlighter-rouge">x</code> (measured in seconds) that goes from 0 to 5 in 15 steps:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_x_i</span><span class="p">(</span><span class="n">from</span><span class="p">,</span> <span class="n">to</span><span class="p">,</span> <span class="n">number_of_steps</span><span class="p">)</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">to</span> <span class="o">-</span> <span class="n">from</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">number_of_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="nf">to_f</span>
  <span class="no">Array</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="n">number_of_steps</span><span class="p">)</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="n">from</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">step_size</span> <span class="p">}</span>
<span class="k">end</span>

<span class="n">x_i</span> <span class="o">=</span> <span class="n">generate_x_i</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</code></pre></div></div> <p>Next we are ready to generate values for <code class="language-plaintext highlighter-rouge">y</code>. We use a function that follows the equation of a vertically thrown ball introduced in the beginning of the article \(h(t)=−4.9t^2+v0​t+h0​\) with an initial speed of <code class="language-plaintext highlighter-rouge">v0=20</code> and a height of <code class="language-plaintext highlighter-rouge">h0=30</code>. We also add some noise to fake imperfect sensor data:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quadratic_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">-</span><span class="mf">4.9</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>

<span class="n">y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">quadratic_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">y</span><span class="o">|</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span><span class="o">*</span><span class="nb">rand</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="o">..</span><span class="mf">0.05</span><span class="p">)</span> <span class="p">}</span>
</code></pre></div></div> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">initial_random_abc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">521</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="o">-</span><span class="mi">120</span><span class="p">]</span>

<span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">run_gradient_descent</span><span class="p">(</span><span class="n">initial_random_abc</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="nb">puts</span> <span class="s2">"Parameters prediction: </span><span class="si">#{</span><span class="n">predicted_parameters</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div></div> <p>and we get:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running iterations.. <span class="o">(</span>100000<span class="o">)</span> with learning rate 0.005
Parameters prediction: <span class="o">[</span><span class="nt">-5</span>.09634353758922, 20.90880952378543, 29.844999999961708]
</code></pre></div></div> <p>Our algorithm predicted the following equation:</p> \[f(x) = -5.09x^2 + 20.91x + 29.85\] <p>When compared to the ball-throwing equation we have on the top, we see how close we got:</p> <ul> <li>Predicted values: \(g = -10.18\), \(v0=20.91\) and \(h0=29.85\)</li> <li>Actual values we used to generate our (noisy) data: \(g = 9.8\), \(v0=20\) and \(h0=30\)</li> </ul> <p>You can easily calculate the MAE value if needed with:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mae</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_parameters</span><span class="p">)</span>
  <span class="n">predicted_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">predicted_parameters</span><span class="p">)</span>
  <span class="n">predicted_y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">predicted_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}</span>
  <span class="n">differences</span> <span class="o">=</span> <span class="n">y_i</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">predicted_y_i</span><span class="p">).</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="o">|</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span><span class="p">).</span><span class="nf">abs</span> <span class="p">}</span>
  <span class="n">differences</span><span class="p">.</span><span class="nf">sum</span> <span class="o">/</span> <span class="n">differences</span><span class="p">.</span><span class="nf">size</span><span class="p">.</span><span class="nf">to_f</span>
<span class="k">end</span>

<span class="nb">puts</span> <span class="s2">"MAE: </span><span class="si">#{</span><span class="n">mae</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_parameters</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div></div> <p>which for our sample run was <code class="language-plaintext highlighter-rouge">MAE: 1.025</code>.</p> <h2 id="conclusion">Conclusion</h2> <p>We have been able to use Gradient Descent to predict the coefficients of a quadratic function with only dirty sample data! We even almost got right the value of gravity! There is a big assumption we’ve done though: we knew our sensor data (throwing a ball vertically) followed a quadratic function.</p> <p>In a next post we will see how we can generalise what we learnt here to resolve more complicated problems like those mentioned in the beginning that don’t necessarily follow a quadratic function with the help of Rectified Linear Units.</p> <p><a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work" rel="external nofollow noopener" target="_blank">Check this Jupyter notebook</a> for interactive learning material or <a href="https://www.youtube.com/watch?v=hBBOjCiFcuo" rel="external nofollow noopener" target="_blank">its’ associated video</a> to grasp better the concepts explained in this article.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/from-ball-to-computer-vision/">From throwing a ball to computer vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/estimate-any-function/">Estimate any function with Gradient Descent</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-bias-and-feedback-loops/">AI Bias and Feedback loops</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fastai-finetuning-a-classifier/">Deploying a fine-tuned classifier</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/simplicity-and-pragmatism/">Simplicity and pragmatism</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="newsletter-form-container"> <form class="newsletter-form" action="https://app.loops.so/api/newsletter-form/cm0teszzw00txtql65tlwc8r9" method="POST" style="justify-content: center"> <input class="newsletter-form-input" name="newsletter-form-input" type="email" placeholder="user@example.com" required=""> <button type="submit" class="newsletter-form-button" style="justify-content: center"> subscribe </button> <button type="button" class="newsletter-loading-button" style="justify-content: center"> Please wait... </button> </form> <div class="newsletter-success" style="justify-content: center"> <p class="newsletter-success-message">You're subscribed!</p> </div> <div class="newsletter-error" style="justify-content: center"> <p class="newsletter-error-message">Oops! Something went wrong, please try again</p> </div> <button class="newsletter-back-button" type="button" onmouseout='this.style.textDecoration="none"' onmouseover='this.style.textDecoration="underline"'> ← Back </button> </div> <script>function submitHandler(e){e.preventDefault();var t=e.target.parentNode,r=t.querySelector(".newsletter-form"),s=t.querySelector(".newsletter-form-input"),l=t.querySelector(".newsletter-success"),n=t.querySelector(".newsletter-error"),o=t.querySelector(".newsletter-error-message"),a=t.querySelector(".newsletter-back-button"),i=t.querySelector(".newsletter-form-button"),y=t.querySelector(".newsletter-loading-button");const d=()=>{n.style.display="flex",o.innerText="Too many signups, please try again in a little while",i.style.display="none",s.style.display="none",a.style.display="block"};var m=(new Date).valueOf(),c=localStorage.getItem("loops-form-timestamp");if(c&&Number(c)+6e4>m)d();else{localStorage.setItem("loops-form-timestamp",m),i.style.display="none",y.style.display="flex";var u="userGroup=&email="+encodeURIComponent(s.value);fetch(e.target.action,{method:"POST",body:u,headers:{"Content-Type":"application/x-www-form-urlencoded"}}).then(e=>[e.ok,e.json(),e]).then(([e,t,s])=>{e?(l.style.display="flex",r.reset()):t.then(e=>{n.style.display="flex",o.innerText=e.message?e.message:s.statusText})})["catch"](e=>{"Failed to fetch"!==e.message?(n.style.display="flex",e.message&&(o.innerText=e.message),localStorage.setItem("loops-form-timestamp","")):d()})["finally"](()=>{s.style.display="none",y.style.display="none",a.style.display="block"})}}function resetFormHandler(e){var t=e.target.parentNode,r=t.querySelector(".newsletter-form-input"),s=t.querySelector(".newsletter-success"),l=t.querySelector(".newsletter-error"),n=t.querySelector(".newsletter-error-message"),o=t.querySelector(".newsletter-back-button"),a=t.querySelector(".newsletter-form-button");s.style.display="none",l.style.display="none",n.innerText="Oops! Something went wrong, please try again",o.style.display="none",r.style.display="flex",a.style.display="flex"}for(var formContainers=document.getElementsByClassName("newsletter-form-container"),i=0;i<formContainers.length;i++){var formContainer=formContainers[i],handlersAdded=formContainer.classList.contains("newsletter-handlers-added");handlersAdded||(formContainer.querySelector(".newsletter-form").addEventListener("submit",submitHandler),formContainer.querySelector(".newsletter-back-button").addEventListener("click",resetFormHandler),formContainer.classList.add("newsletter-handlers-added"))}</script> <noscript> <style>.newsletter-form-container{display:none}</style> </noscript> <div class="container"> © Copyright 2024 Gerovlabs ltd - sava [AT] gerov.es </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script>$(document).ready(function(){var t=null,a=null,e=null,n="";$(".language-chartjs").each(function(){a=$(this),t=$("<canvas></canvas>"),n=a.text(),a.text("").append(t),(e=t.get(0).getContext("2d"))&&n&&new Chart(e,JSON.parse(n))&&a.attr("data-processed",!0)})});</script> <script src="/assets/js/typograms.js?63f3caa50c7a9624f953b3aec207afa6"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-typograms").forEach(e=>{const t=e.textContent,n=e.parentElement.parentElement;let a=document.createElement("pre");a.classList.add("typogram");const d=create("\n"+t,.3,!1);a.appendChild(d),n.appendChild(a),n.removeChild(e.parentElement)})});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-LW56FFFC4E"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-LW56FFFC4E");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-learning-hub",title:"learning hub",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-from-throwing-a-ball-to-computer-vision",title:"From throwing a ball to computer vision",description:"Let&#39;s look at how neural networks allow to solve predictions beyond quadratic functions.",section:"Posts",handler:()=>{window.location.href="/blog/from-ball-to-computer-vision/"}},{id:"post-estimate-any-function-with-gradient-descent",title:"Estimate any function with Gradient Descent",description:"Let&#39;s have a look how we can resolve more generic functions with Gradient Descent by combining linear functions and non-linear ones.",section:"Posts",handler:()=>{window.location.href="/blog/estimate-any-function/"}},{id:"post-gradient-descent-from-scratch",title:"Gradient Descent from scratch",description:"A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample.",section:"Posts",handler:()=>{window.location.href="/blog/gradient-descent-from-scratch/"}},{id:"post-ai-bias-and-feedback-loops",title:"AI Bias and Feedback loops",description:"On how AI imposes a danger and further perpetuates human flaws.",section:"Posts",handler:()=>{window.location.href="/blog/ai-bias-and-feedback-loops/"}},{id:"post-deploying-a-fine-tuned-classifier",title:"Deploying a fine-tuned classifier",description:"Leveraging fast.ai capabilities for a quick resnet model fine-tuning and delivery.",section:"Posts",handler:()=>{window.location.href="/blog/fastai-finetuning-a-classifier/"}},{id:"post-simplicity-and-pragmatism",title:"Simplicity and pragmatism",description:"The power of doing less.",section:"Posts",handler:()=>{window.location.href="/blog/simplicity-and-pragmatism/"}},{id:"post-integrity-and-transparency",title:"Integrity and transparency",description:"By being upfront about risks, challenges, and solutions we earn the trust of clients, stakeholders, and team.",section:"Posts",handler:()=>{window.location.href="/blog/integrity-and-transparency/"}},{id:"post-ethical-responsibility",title:"Ethical responsibility",description:"The systems we design and the software we create shape industries, impact individuals, and define modern society.",section:"Posts",handler:()=>{window.location.href="/blog/ethical-responsibility/"}},{id:"post-delivering-value-over-process",title:"Delivering value over process",description:"The real goal of any software project should be rather simple - delivering value.",section:"Posts",handler:()=>{window.location.href="/blog/deliver-value/"}},{id:"post-collaboration-and-communication",title:"Collaboration and communication",description:"Managing expectations and ensuring alignment across and within teams.",section:"Posts",handler:()=>{window.location.href="/blog/collaboration-and-communication/"}},{id:"post-continuous-learning-and-adaptability",title:"Continuous learning and adaptability",description:"In software engineering, things change fast and often.",section:"Posts",handler:()=>{window.location.href="/blog/continuous-learning/"}},{id:"post-technical-excellence",title:"Technical excellence",description:"Building high-quality systems that last.",section:"Posts",handler:()=>{window.location.href="/blog/technical-excellence/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>