<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://www.gerovlabs.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.gerovlabs.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-04T02:30:17+00:00</updated><id>https://www.gerovlabs.com/feed.xml</id><title type="html">GerovLabs</title><subtitle>GerovLabs is a Custom software development, technical leadership and consulting company. </subtitle><entry><title type="html">From throwing a ball to computer vision</title><link href="https://www.gerovlabs.com/blog/from-ball-to-computer-vision/" rel="alternate" type="text/html" title="From throwing a ball to computer vision"/><published>2024-11-02T00:00:00+00:00</published><updated>2024-11-02T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/from-ball-to-computer-vision</id><content type="html" xml:base="https://www.gerovlabs.com/blog/from-ball-to-computer-vision/"><![CDATA[<p>Now that we’ve covered <a href="/blog/gradient-descent-from-scratch/">how gradient descent works</a> and <a href="/blog/estimate-any-function/">how we can estimate any function</a> by combining linear and non-linear functions we are equipped to resolve some more interesting problems. Building on top of the <a href="/blog/gradient-descent-from-scratch/">samples in the first post of this series</a> we will look at the problem of identifying what number appears in an image:</p> <pre><code class="language-typograms">      picture     +--------+ identify number
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+
</code></pre> <h3 id="digit-classifier-with-linear-functions">Digit classifier with linear functions</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/beyond-3-480.webp 480w,/assets/img/beyond-3-800.webp 800w,/assets/img/beyond-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/beyond-3.png" class="img-fluid rounded z-depth-1 w-md-30 float-md-right ml-md-2" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s start with a simple example such as a fitting a function that tells us if a number is a 5 or not based on the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a> (a database of handwritten numbers in image format).</p> <p>The first question that comes to mind is how do we represent a <code class="language-plaintext highlighter-rouge">28x28</code> pixels image so that our linear functions can understand it. We can rely on Python PIL and PyTorch which allow us to convert an input image to its tensor representation. Since the MNIST database stores images in grayscale (<code class="language-plaintext highlighter-rouge">L</code> mode) each pixel will be a number ranging from 0 to 255.</p> <p>We can build on top of <a href="/blog/estimate-any-function/">our last article</a> sample and consider each pixel of a 28x28 (784 pixels) image to be a separate input:</p> <p><br/></p> <pre><code class="language-typograms">+---------+ x weight1                 + bias    +--------+
| pixel 1 |-------------------------+----------&gt;| output |
+---------+                         |           +--------+
                                    |
+---------+ ...                     |
| ...     |-------------------------+
+---------+                         |
                                    |
+-----------+ x weight784           |
| pixel 784 |-----------------------+
+-----------+
</code></pre> <p><strong>Data preparation</strong></p> <p>First we need to load our <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> values. In the prior article we just made up some <code class="language-plaintext highlighter-rouge">x</code> values in a range and got the <code class="language-plaintext highlighter-rouge">y</code>s by applying some quadratic function. This time we need some images as <code class="language-plaintext highlighter-rouge">x</code> and wether they are a 5 or not as <code class="language-plaintext highlighter-rouge">y</code>. We can set our targets (<code class="language-plaintext highlighter-rouge">y</code> values) to be either a <code class="language-plaintext highlighter-rouge">1</code> if the picture is a 5 or <code class="language-plaintext highlighter-rouge">0</code> if it isn’t so our Neural Network would be trying to approximate to either <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code> given an image, e.g.:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># a picture that is not a five followed by two that are 5s
</span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span> <span class="c1"># a made-up prediction given by a neural network
</span></code></pre></div></div> <p>We can rely on fast.ai to load our images, a sample for the 5s would look like the following but we could load any number from MNIST the same way:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="nf">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="p">.</span><span class="n">MNIST</span><span class="p">)</span> 
<span class="n">fives_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">fives_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">fives_filenames</span><span class="p">]</span>
<span class="n">fives</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">fives_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
</code></pre></div></div> <blockquote> <p>Note that as a best practice we are normalising the pixel values to numbers between 0 and 1 instead of having to handle numbers from 0 to 255. It stabilizes the training by keeping inputs within a smaller range and far from the extremes of activation functions while also allowing to load images from distinct formats</p> </blockquote> <p>We can now load all the images we want to be able to recognise, let’s start only with 5s and 4s for the sake of simplicity:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">fives</span><span class="p">,</span> <span class="n">fours</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">fives</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">fours</span><span class="p">)).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>We are using the <code class="language-plaintext highlighter-rouge">view</code> and <code class="language-plaintext highlighter-rouge">unsqueeze</code> functions to ensure our tensors have the right dimensions:</p> <ul> <li>for <code class="language-plaintext highlighter-rouge">x</code> we make sure each image is a single dimension so we can input it in our linear functions and</li> <li>for <code class="language-plaintext highlighter-rouge">y</code> we make sure each output is in its own row so that we can use broadcasting in our loss and accuracy functions later on</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span>
<span class="c1"># (torch.Size([11263, 784]), torch.Size([11263, 1]))
</span></code></pre></div></div> <p><strong>Loss function</strong></p> <p>Now we need a loss function for our gradient descent implementation. <a href="/blog/estimate-any-function/">In our prior article</a> we used <code class="language-plaintext highlighter-rouge">F.l1_loss</code> function to calculate the <em>Absolute Mean Error</em> between all predictions and the actual outputs of the data we were approximating. Now we aren’t comparing two real values distance but how far from a 5 are we, so we need a loss function that can handle binary classification instead of continuous regression.</p> <p>One possible loss function would be to have a function that counts the number of 1s in the actual data (targets) and compare it to our predicitons (did we estimate the same 5s?):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># broadcasting PyTorch tensors to count correct predictions
</span><span class="n">corrects</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="o">&gt;</span><span class="mf">0.0</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">==</span> <span class="n">targets</span> 
<span class="c1"># convert booleans to 0/1 and take the mean to know how many 1s we got
</span><span class="n">corrects</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span> 
</code></pre></div></div> <p>That would make a great metric to know the accuracy of our system - but not a great loss function. Tiny changes in the weights will only change the final result when a prediction flips from 0 to 1 or viceversa so the gradient would be constant most of the time and gradient descent would struggle to optimise the weights.</p> <p>An alternative approach would be to quantify how far we are from the 1s and 0s instead of just counting, hence optimising for either <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code> classifications:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>By relying on the <a href="https://pytorch.org/docs/stable/generated/torch.where.html">torch where function</a> we can calculate a loss function that is more sensible to changes in the weights than just counting the number of <code class="language-plaintext highlighter-rouge">1</code>s because a tiny changes would be accounted for. E.g. if some particular image prediction should be marked as a 5 the prediction value should be <code class="language-plaintext highlighter-rouge">1</code>. If our Neural Network predicts <code class="language-plaintext highlighter-rouge">0.6</code> instead of <code class="language-plaintext highlighter-rouge">0.5</code> it got a bit closer to the real value (that wouldn’t happen with the accuracy function described in the first attempt).</p> <p>One major problem of the loss function above is that it assumes that our Neural Network is predicting values between 0 and 1 which won’t be the case since we aren’t applying any non-linear function at the end of the NN. A simple solution to the problem would be to apply <code class="language-plaintext highlighter-rouge">predictions.sigmoid()</code> to the tensor before the loss function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mnist_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">targets</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Gradient Descent</strong></p> <p>With that we are ready to implement basic gradient descent as we did in <a href="/blog/estimate-any-function/">the last article</a>. The only difference is that we are using 784 (28x28) input parameters instead of 1 and adjusted the names to be closer to neural networks jargon:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="nd">@weights</span> <span class="o">+</span> <span class="n">bias</span>

<span class="k">def</span> <span class="nf">calc_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mnist_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="nf">calc_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">p</span><span class="p">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="o">*</span><span class="n">lr</span>
        <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span> <span class="nf">return </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="n">std</span><span class="p">).</span><span class="nf">requires_grad_</span><span class="p">()</span>
</code></pre></div></div> <p>With that we are ready to run some epochs (in this case we are running through all data each epoch so each epoch has only 1 gradient descent step):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">weights</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">((</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">bias</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">weights</span><span class="p">,</span><span class="n">bias</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">train_epoch</span><span class="p">(</span><span class="n">linear1</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>Which results in the following loss reduction curve:</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/beyond-1-480.webp 480w,/assets/img/beyond-1-800.webp 800w,/assets/img/beyond-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/beyond-1" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>That doesn’t really tell us how will our model perform but tells us how well Gradient Descent is doing with the loss function we have defined on top of our training data. To have a better understanding of how our model behaves and generalises we need to check with data it has never seen before (validation set) and look at its accuracy metrics (not loss).</p> <p>We can leverage the validation set given in the MNIST dataset and execute the accuracy calculation lines we looked at above:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># exactly same data loading lines as for our training set
</span><span class="n">validation_fives_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">testing</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">5</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">validation_fives_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">validation_fives_filenames</span><span class="p">]</span>
<span class="n">validation_fives</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">validation_fives_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">validation_fours_filenames</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="sh">'</span><span class="s">testing</span><span class="sh">'</span><span class="o">/</span><span class="sh">'</span><span class="s">4</span><span class="sh">'</span><span class="p">).</span><span class="nf">ls</span><span class="p">().</span><span class="nf">sorted</span><span class="p">()</span>
<span class="n">validation_fours_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tensor</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">validation_fours_filenames</span><span class="p">]</span>
<span class="n">validation_fours</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">validation_fours_tensors</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">validation_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">validation_fives</span><span class="p">,</span> <span class="n">validation_fours</span><span class="p">]).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">validation_y</span> <span class="o">=</span> <span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">validation_fives</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">validation_fours</span><span class="p">)).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># accuracy calculation
</span><span class="n">predictions</span> <span class="o">=</span> <span class="nf">linear1</span><span class="p">(</span><span class="n">validation_x</span><span class="p">)</span>
<span class="n">corrects</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="o">&gt;</span><span class="mf">0.0</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">==</span> <span class="n">validation_y</span> 
<span class="n">corrects</span><span class="p">.</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>Which results in an accuracy of <strong>93.01%</strong>.</p> <h3 id="adding-a-non-linear-function">Adding a non-linear function</h3> <p>A natural next step as explained in <a href="/blog/estimate-any-function/">the last article</a> is to introduce non-linearity between our input and our output for further prediction flexibility:</p> <pre><code class="language-typograms">+---------+ x weight1 + bias1             +------+ x weight + bias  +--------+
| pixel 1 |-------------------------+----&gt;| Sigm |---------+-------&gt;| output |
+---------+                         |     +------+         |        +--------+
                                    |                      |
+---------+ ...                     |     +------+         |
| ...     |-------------------------+----&gt;| Sigm |---------+
+---------+                         |     +------+         |
                                    |                      |
+-----------+ x weight784 + bias784 |     +------+         |
| pixel 784 |-----------------------+----&gt;| Sigm |---------+
+-----------+                             +------+
</code></pre> <p>What that would imply for our code is that we would need to replace our <code class="language-plaintext highlighter-rouge">linear1</code> function with:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simple_neural_net</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="n">res</span> <span class="o">=</span> <span class="n">x</span><span class="nd">@w1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="nd">@w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></div> <p>and our gradient descent code becomes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">((</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">((</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b2</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="nf">train_epoch</span><span class="p">(</span><span class="n">simple_neural_net</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>Notice that <code class="language-plaintext highlighter-rouge">w1</code> is a matrix of <code class="language-plaintext highlighter-rouge">784x784</code> rows per columns instead of <code class="language-plaintext highlighter-rouge">784x1</code> as we did above. That’s because we have added a Sigmoid for each single pixel in our sample. We could have added any other number of Sigmoids in our hidden layer. If that’s confusing it might be worth to <a href="https://www.youtube.com/watch?v=lFOOjeH2wsY">check out this video</a>.</p> <p>After running the code with non-linearity we get the following loss curve which has a better performance:</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/beyond-2-480.webp 480w,/assets/img/beyond-2-800.webp 800w,/assets/img/beyond-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/beyond-2" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The only line we switched is:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-predictions = linear1(validation_x)
</span><span class="gi">+predictions = simple_neural_net(validation_x)
</span></code></pre></div></div> <p>Running the accuracy for this sample we go up to <strong>95.3%</strong> which is already pretty good knowing how little we’ve been training this very simple neural network.</p> <p>The fact that we added the same number of nodes as input parameters is random. Usually they won’t match since the inputs might be way too many. Since we are looking at a fully connected neural network each node connects to all inputs so it would represent a learnt feature of a particular combination of pixels 784 pixels. If we reduce the nodes to 50 instead, we get an accuracy of <strong>94%</strong>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">((</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b2</span> <span class="o">=</span> <span class="nf">init_params</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>In next posts we will explore Stochastic Gradient Descent and how we can leverage more Pytorch and Fast.ai libraries to simplify our code. For more information look at <a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">chapter 4 of the fast.ai book</a> or its associated <a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">Jupyter notebooks</a>.</p>]]></content><author><name></name></author><category term="exploration"/><category term="ai"/><summary type="html"><![CDATA[Let's look at how neural networks allow to solve predictions beyond quadratic functions.]]></summary></entry><entry><title type="html">Estimate any function with Gradient Descent</title><link href="https://www.gerovlabs.com/blog/estimate-any-function/" rel="alternate" type="text/html" title="Estimate any function with Gradient Descent"/><published>2024-10-23T00:00:00+00:00</published><updated>2024-10-23T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/estimate-any-function</id><content type="html" xml:base="https://www.gerovlabs.com/blog/estimate-any-function/"><![CDATA[<p>We already described in <a href="/blog/gradient-descent-from-scratch/">Gradient Descent from scratch</a> how we can find all 3 parameters of a single dimension quadratic function (ball height <code class="language-plaintext highlighter-rouge">h</code> for time <code class="language-plaintext highlighter-rouge">t</code>). Let’s now extend on that knowledge to see if we can optimise parameters with no prior knowledge of the underlying function of our data.</p> <p>The function we used in our prior post was \(h(t)=−4.9t^2+20​t+30​\). Let’s try to approximate it in a generic way!</p> <h3 id="first-attempt">First attempt</h3> <p>We want to find the relationship between two one-dimension variables (<code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>). It’s simplest form would be a <em>constant relationship</em>, i.e. for all values of <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">f(x)</code> does not change: \(f(x) = K\) where <code class="language-plaintext highlighter-rouge">K</code> can by any constant value. Most input data we want to predict isn’t constant values so that won’t work (our quadratic might hit the same <code class="language-plaintext highlighter-rouge">y</code> twice at most due to its simmetry but not more than that).</p> <pre><code class="language-typograms">       +----------+
------&gt;| constant |------&gt;
   x   +----------+   K
</code></pre> <p>As a second guess we can try a <em>linear relationship</em> where <code class="language-plaintext highlighter-rouge">y</code> changes linearly with <code class="language-plaintext highlighter-rouge">x</code>, i.e. \(f(x) = x\). Such a line has a slope of <code class="language-plaintext highlighter-rouge">1</code> and passes through the origin (<code class="language-plaintext highlighter-rouge">y=0</code> when <code class="language-plaintext highlighter-rouge">x=0</code>) (<a href="https://www.youtube.com/watch?v=lz8zVJxRFX8">more on lines here</a>) so let’s parameterise it so we can move it around and be a bit more flexible:</p> <p>\(f(x) = m*x + b\) where <code class="language-plaintext highlighter-rouge">m</code> is our slope and <code class="language-plaintext highlighter-rouge">b</code> is the Y-intercept</p> <pre><code class="language-typograms">       +--------+
------&gt;| linear |------&gt;
   x   +--------+   y
</code></pre> <blockquote> <p>Note: if the slope is 0 (<code class="language-plaintext highlighter-rouge">m=0</code>) it would be equivalent to having a constant as in our first idea, i.e. a parallel line to the <code class="language-plaintext highlighter-rouge">x</code> axis</p> </blockquote> <p>This time we’ll rely on PyTorch to calculate the gradients so we don’t need to manually take derivatives for our loss function like we did <a href="/blog/gradient-descent-from-scratch/">in the previous post</a>.</p> <p>Let’s generate some data for the function we want to find (no noise in our <code class="language-plaintext highlighter-rouge">y</code> values this time for the sake of simplicity):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_f</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="mf">4.9</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">20</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">30</span><span class="p">;</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</code></pre></div></div> <p>And execute GD for a simple line:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">line</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
<span class="k">def</span> <span class="nf">generate_line</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> <span class="k">return</span> <span class="nf">partial</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">;</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>

<span class="n">params</span> <span class="o">=</span> <span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]).</span><span class="nf">float</span><span class="p">();</span>
<span class="n">params</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">();</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">predicted_f</span> <span class="o">=</span> <span class="nf">generate_line</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
    <span class="n">predicted_y</span> <span class="o">=</span> <span class="nf">predicted_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">l1_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="n">params</span> <span class="o">-=</span> <span class="n">params</span><span class="p">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">learning_rate</span>
    <span class="n">params</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
</code></pre></div></div> <p>If we look at the behaviour of the loss in each iteration, we’ll see that we aren’t getting close enough to 0 (<code class="language-plaintext highlighter-rouge">MAE = 30.6</code>):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-closest-0relu-480.webp 480w,/assets/img/gd-closest-0relu-800.webp 800w,/assets/img/gd-closest-0relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-closest-0relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-loss-0relu-480.webp 480w,/assets/img/gd-loss-0relu-800.webp 800w,/assets/img/gd-loss-0relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-loss-0relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Closest line to our input data by using Gradient Descent </div> <p>We could tweak our learning rate or steps number but will hardly lower much more the error rate since there is only so much we can do with a plain line.</p> <h3 id="second-attempt">Second attempt</h3> <p>Let’s see what happens if we apply a simple non-linear function on the output of the linear function we just tried. One such function is <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">the ReLu</a> which converts negative outputs to 0:</p> \[ReLu(x) = {\begin{cases}x&amp;{\text{if }}x&gt;0,\\0&amp;{\text{otherwise}},\end{cases}}\] <p>So we end up with a composite function like this:</p> <pre><code class="language-typograms">       +--------+        +------+
------&gt;| linear |-------&gt;| ReLu |------&gt;
   x   +--------+   y'   +------+   y
</code></pre> <p>Let’s run GD for this function, where only the optimised function changes:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-predicted_y = predicted_f(x)
</span><span class="gi">+predicted_y = F.relu(predicted_f(x))
</span></code></pre></div></div> <p>We actually worsened the loss adding the non-linear component in our example (MAE = 49.5) but something very powerful happened. The ReLu allowed GD to come up with a more complex shape that represents our input data:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-closest-1relu-480.webp 480w,/assets/img/gd-closest-1relu-800.webp 800w,/assets/img/gd-closest-1relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-closest-1relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-loss-1relu-480.webp 480w,/assets/img/gd-loss-1relu-800.webp 800w,/assets/img/gd-loss-1relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-loss-1relu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Closest rectified line to our input data by using Gradient Descent </div> <h3 id="further-refinements">Further refinements</h3> <p>As we can see just by adding a non-linear function we use we can allow Gradient Descent to come up with more flexible functions. Since the ReLu crops values below 0, we could add a horizontal line with no slope (<code class="language-plaintext highlighter-rouge">m=0</code>) to allow Gradient Descent to find values that would move our predictions in the negatives of the Y-axis:</p> <pre><code class="language-typograms">       +--------+        +------+       +--------------+
------&gt;| linear |-------&gt;| ReLu |------&gt;| linear (m=0) |------&gt;
   x   +--------+   y''  +------+   y'  +--------------+   y
</code></pre> <p>The only change in our code would be to add an additional parameter from the new line \(f(x) = 0*x + b\):</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">-predicted_f = generate_line(*params)
-predicted_y = predicted_f(x)
</span><span class="gi">+predicted_line_1 = generate_line(params[0], params[1])
+predicted_line_2 = generate_line(0, params[2])
+predicted_y = F.relu(predicted_line_1(x)) + predicted_line_2
</span></code></pre></div></div> <p>Which would result in the following prediction:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-closest-1relu-bias-480.webp 480w,/assets/img/gd-closest-1relu-bias-800.webp 800w,/assets/img/gd-closest-1relu-bias-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-closest-1relu-bias.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gd-loss-1relu-bias-480.webp 480w,/assets/img/gd-loss-1relu-bias-800.webp 800w,/assets/img/gd-loss-1relu-bias-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gd-loss-1relu-bias.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Closest rectified line with bias to our input data by using Gradient Descent </div> <p>Notice how GD was able to figure out that it needed a negative Y-intercept for the last line we introduced so that our predictions would go down in the Y-axis.</p> <p>With ths foundation we are able to increase the number of rectified linear units enough so that <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">we can draw any function</a> that resembles our input data (e.g. think of a voice waveform). It would be a matter of performing more computations on our errror minimisation function (while calculating the gradients of each parameter in each step) among other tricks.</p> <p>The exact same process applies if we increased the number of independent variables (or input dimensions) for the function we are trying to find. It just becomes harder to visualise since we are going beyond the 2-D we are used to seeing in a coordinate plane (our output Y versus our input X).</p> <p>This is a sample 3-D plot where a ReLu is applied on a 2 dimensional input (<code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>). The main difference with a single dimension is that now Gradient Descent will be looking on more axes to find its way through the planes (if functions are linear). Check <a href="https://www.youtube.com/watch?v=2DRmfxkH_VI&amp;list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&amp;index=3">Khans’ Academy introduction to 3D graphs</a> if you need help with multivariable visualisation.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/relu-3d-480.webp 480w,/assets/img/relu-3d-800.webp 800w,/assets/img/relu-3d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/relu-3d" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> ReLu with 2 dimensions (plot by <a href="https://towardsdatascience.com/how-a-neuron-in-a-2d-artificial-neural-network-bends-space-in-3d-visualization-d234e8a8374e">Avinash Dubey</a>) </div> <p>We can abstract the Line + ReLu we implemented above and add some Neural Networks terms:</p> <ul> <li><strong>Weight</strong>: the slope <code class="language-plaintext highlighter-rouge">m</code> of our linear function</li> <li><strong>Bias</strong>: the y-intercept <code class="language-plaintext highlighter-rouge">b</code> of our linear function</li> <li><strong>Activation function</strong>: in our case, we chose a <code class="language-plaintext highlighter-rouge">ReLu</code></li> <li><strong>Node</strong>: applies an activation function on the weighted sum of its inputs</li> <li><strong>Connection</strong>: the lines connecting the nodes which apply a linear function</li> </ul> <pre><code class="language-typograms">.-------. x weight1 + bias1   .------. x weight3 + bias3  .--------.
| input |--------------------&gt;| node |-------------------&gt;| output |
.-------.                   | .------.                    .--------.
                            |
.-------. x weight2 + bias2 |
| input |-------------------+
.-------.
</code></pre> <p>Since our node sums all weighted inputs, we could represent all biases sums with an additional input with a weight of 1. In our sample we have a final linear function (and bias) to adjust for our negative output values which would remain the same:</p> <pre><code class="language-typograms">.-------. x weight1           .------. x 0 + bias1        .--------.
| input |--------------------&gt;| node |-------------------&gt;| output |
.-------.                   | .------.                    .--------.
                            |
.-------. x weight2         |
| input |-------------------+
.-------.                   | 
                            |
.------.  x 1               |
| bias |--------------------+
.------.
</code></pre> <p>Now we have seen some building blocks and how they work in our first Neural Network sample which equips us to resolve way more complex problems in a more generic fashion!</p>]]></content><author><name></name></author><category term="exploration"/><category term="ai"/><summary type="html"><![CDATA[Let's have a look how we can resolve more generic functions with Gradient Descent by combining linear functions and non-linear ones.]]></summary></entry><entry><title type="html">Gradient Descent from scratch</title><link href="https://www.gerovlabs.com/blog/gradient-descent-from-scratch/" rel="alternate" type="text/html" title="Gradient Descent from scratch"/><published>2024-10-16T00:00:00+00:00</published><updated>2024-10-16T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/gradient-descent-from-scratch</id><content type="html" xml:base="https://www.gerovlabs.com/blog/gradient-descent-from-scratch/"><![CDATA[<p>The following article describes how Gradient Descent works which lies at the core of Neural Networks. Don’t worry like I did, only basic Algebra and Calculus are needed to understand well the underlying math and I’ll go through it in detail.</p> <h2 id="problem-statement">Problem statement</h2> <p>Let’s start defining what are we trying to achieve. Given an input, we want to find a function <code class="language-plaintext highlighter-rouge">f(x)</code> that would predict an output with enough accuracy for it to be useful for us as humans.</p> <pre><code class="language-typograms"> input +--------+ output
------&gt;|  f(x)  |-------&gt;
   x   +--------+   y
</code></pre> <p>Let’s look at some samples of some functions we might be interested in:</p> <pre><code class="language-typograms">       time       +--------+  height of ball
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

      picture     +--------+ identify number
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

    audio song    +--------+      genre
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+

       text       +--------+      topic
-----------------&gt;|  f(x)  |----------------&gt;
                  +--------+
</code></pre> <p>The outputs depend on the inputs and we can find a mathematical function that relates the two. Finding what that function is would allow us to get an output prediction for each input as long as the function resembles the real world closely enough!</p> <p>And since math functions do represent real world phenomenons, let’s just use one such example. The following quadratic function tells us the height of a thrown ball upwards for a particular time <code class="language-plaintext highlighter-rouge">t</code>:</p> \[h(t)=−\frac 1 2 gt^2+v0​t+h0​\] <p>where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">g</code> is gravity on earth level</li> <li><code class="language-plaintext highlighter-rouge">v0</code> is the initial throwing speed</li> <li><code class="language-plaintext highlighter-rouge">h0</code> is the initial height of the throw</li> </ul> <p>If we wanted to predict the height at a particular time (e.g. after 2 seconds) we just need to know the initial throwing speed <code class="language-plaintext highlighter-rouge">v0</code> and height <code class="language-plaintext highlighter-rouge">h0</code> and we can easily resolve <code class="language-plaintext highlighter-rouge">h(2)</code> and get our answer.</p> <p>What if we don’t know the actual formula nor initial throwing/height speed but only have observations from nature? Let’s say a ball was thrown vertically and we installed a sensor which gave us 15 measurements of all the heights the ball went through during 5 seconds:</p> <pre><code class="language-chartjs">{
  "type": "scatter",
  "data": {
    "labels": [0.0, 0.35714285714285715, 0.7142857142857143, 1.0714285714285714, 1.4285714285714286, 1.7857142857142858, 2.142857142857143, 2.5, 2.857142857142857, 3.2142857142857144, 3.5714285714285716, 3.928571428571429, 4.285714285714286, 4.642857142857143, 5.0],
    "datasets": [
      {
        "label": "Height of thrown ball",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [29.128620665942556, 35.24960579584343, 42.16851569710115, 44.90160627870316, 46.63155173360897, 52.22662725576406, 52.86940417603709, 50.56015123210324, 46.128434083285384, 44.53718441643712, 40.57409445955629, 32.19438620635332, 24.50774696452207, 17.09045405656899, 7.26565053601106],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x = time in seconds"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y = height in meters"
        }
      }
    }
  }
}
</code></pre> <p>In this example we want to be able to predict where the ball will be at a certain point in time without knowing the <code class="language-plaintext highlighter-rouge">h(t)</code> formula. For now let’s assume the phenomenon follows some quadratic function:</p> \[f(x)=ax^2 + bx + c\] <p>So we are looking for coefficients <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> that will produce values as similar as possible to the input data we have provided through our sensors.</p> <p>For example, if <code class="language-plaintext highlighter-rouge">a = 3</code>, <code class="language-plaintext highlighter-rouge">b = 2</code>, <code class="language-plaintext highlighter-rouge">c = 1</code> we would have \(f(x)=3x^2 + 2x + 1\). Take one of our real inputs (e.g. <code class="language-plaintext highlighter-rouge">x = 0.714</code>), and we see that \(f(0.714) = 3.957388\). From our input data we know that <code class="language-plaintext highlighter-rouge">y = 42.169</code> for that value of <code class="language-plaintext highlighter-rouge">x</code> so the difference (or error) of our prediction is \(y - f(x)\) which is <code class="language-plaintext highlighter-rouge">42.169 - 3.957388 = 38.211612</code>. We are still far from having a function that predicts well our ball trajectory!</p> <p>Our goal is to minimise the error between our input data and a quadratic function we are looking for (by choosing <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> coefficients).</p> <p>How can we measure the error of the whole function and not for a single point only?</p> <h2 id="calculating-errors">Calculating errors</h2> <p>For a single point we relied on their distance \(y - f(x)\). We want to know the error across all input data we have so we can properly evaluate if our function is good enough. We can achieve that by just taking the mean of all individual errors! For convenience, instead of using <em>function notation</em> we can use our dependant variable name (like we do with <code class="language-plaintext highlighter-rouge">y</code>), i.e. \(\hat y = f(x)\). Our mean would look like this:</p> \[\frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)\] <p>where <code class="language-plaintext highlighter-rouge">n</code> goes from the first sensor datapoint to the last (0 to 15 in our example). The issue with this function is that our predictions might also be negative (if \(\hat y_i &gt; y_i\)) which would screw the mean. We need a more resilient function and a common way to bypass that problem is to use the square of a number since a squared negative result would become positive:</p> \[MSE = \frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)^2\] <p>That is the <strong>Mean Square Error (MSE)</strong> function. It has a problem though, squaring each number implies that our error will be way higher than the actual <code class="language-plaintext highlighter-rouge">y</code> values of our function. In our example, our error was <code class="language-plaintext highlighter-rouge">38.211612</code> which when squared already goes up to <code class="language-plaintext highlighter-rouge">1460.12729164</code>. If we aren’t willing to handle such big numbers in our errors we can take the square root on top of our error which would reduce our number again:</p> \[RMSE = \sqrt {\frac 1 n \sum _{i=1} ^n (y_i - \hat y_i)^2}\] <p>That is the <strong>Root Mean Square Error (RMSE)</strong> function.</p> <p>One of the main issues of both functions is the squaring process which has sensitivity to outliers (bigger numbers will be way bigger, numbers close to 0 will be way smaller). To avoid the issue, we could get the absolute value of the difference instead:</p> \[MAE = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>That is the <strong>Mean Absolute Error (MAE)</strong> function and we will be using it to calculate how far are we with our predictions from the sensor values. One of its major drawbacks is that it’s not differentiable and we’ll explain shortly why is that important.</p> <p>Now that both our problem statement and measure of success are clear we need a resolution method!</p> <h2 id="solution">Solution</h2> <p>In the sample above we chose random values for <code class="language-plaintext highlighter-rouge">a = 3</code>, <code class="language-plaintext highlighter-rouge">b = 2</code> and <code class="language-plaintext highlighter-rouge">c = 1</code> which results in an error of <code class="language-plaintext highlighter-rouge">MAE=32.98</code>. The lower the error, the better we are predicting our input data. How can we adjust <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> iteratively while minimising the error?</p> <p>One rather tedious approach would be to adjust the coeficients randomly and watch how <code class="language-plaintext highlighter-rouge">MAE</code> changes with each coefficient adjustment (for the better or the worst) until we achieve to reduce the <code class="language-plaintext highlighter-rouge">MAE</code> enough. Luckily there is a much better approach through differentiation.</p> <h3 id="differentiation">Differentiation</h3> <p>The question that has just arised is how much would a function change by a change on one of its parameters. In particular, we are asking how much would <code class="language-plaintext highlighter-rouge">MAE</code> output change by a slight change on one of its parameters (e.g. <code class="language-plaintext highlighter-rouge">a</code>).</p> <p>Let’s start with a simpler example, a line \(f(x) = x\):</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [0,1, 2],
    "datasets": [
      {
        "label": "x",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [0, 1, 2],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>By looking at the plot it’s easy to conclude that for each unit increment of <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">y</code> would increment by 1. This is also known as the <em>slope</em> of the tangent line at that point (or <em>rate of change</em>) and it’s described by \(slope = \frac {rise} {run}\), i.e. \(slope = \frac {y_2 - y_1} {x_2 - x_1}\).</p> <p>Let’s now have a look at a more complicated example, a non-linear function like \(f(x) = x^2\):</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [-4, -3, -2, -1, 0, 1, 2, 3, 4],
    "datasets": [
      {
        "label": "x^2",
        "fill": false,
        "lineTension": 0.3,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "bevel",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 0,
        "pointRadius": 0,
        "pointHitRadius": 10,
        "data": [16, 9, 4, 1, 0, 1, 4, 9, 16],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>Here it becomes slightly trickier to know by how much would <code class="language-plaintext highlighter-rouge">y</code> change if <code class="language-plaintext highlighter-rouge">x</code> changed by one because depending on where you look you would get a different slope due to the lack of linearity in our function. For example from <code class="language-plaintext highlighter-rouge">x=4</code> to <code class="language-plaintext highlighter-rouge">x=3</code> it’s \(\frac {16 - 9} {4 - 3} = 7\) but from <code class="language-plaintext highlighter-rouge">x=3</code> to <code class="language-plaintext highlighter-rouge">x=2</code> we get \(\frac {9 - 4} {3 - 2} = 5\).</p> <p>Here is where differentiation kicks in! If we zoom in the slope for <code class="language-plaintext highlighter-rouge">x</code> further then <code class="language-plaintext highlighter-rouge">3</code> to <code class="language-plaintext highlighter-rouge">2</code> and look at values closer to <code class="language-plaintext highlighter-rouge">2</code> we would be getting closer to the real slope of the tangent line in that point (<code class="language-plaintext highlighter-rouge">x=2</code>):</p> <ul> <li><code class="language-plaintext highlighter-rouge">3.000</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {9 - 4} {3 - 2} = 5\)</li> <li><code class="language-plaintext highlighter-rouge">2.500</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {6.25 - 4} {2.5 - 2} = 4.5\)</li> <li><code class="language-plaintext highlighter-rouge">2.250</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {5.0625 - 4} {2.25 - 2} = 4.25\)</li> <li><code class="language-plaintext highlighter-rouge">2.010</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {4.0401 - 4} {2.01 - 2} = 4.01\)</li> <li><code class="language-plaintext highlighter-rouge">2.001</code> to <code class="language-plaintext highlighter-rouge">2</code>: \(\frac {4.004001 - 4} {2.001 - 2} = 4.001\)</li> </ul> <p>Looks like we are approaching the slope of <code class="language-plaintext highlighter-rouge">4</code>! We could try the same process from <code class="language-plaintext highlighter-rouge">1</code> to <code class="language-plaintext highlighter-rouge">2</code> and we would be approaching the same number. Since <code class="language-plaintext highlighter-rouge">y = f(x)</code> we could express the difference (or <em>slope</em> of tangent line) of any the two points like this:</p> \[\frac {f(x_2) - f(x_1)} {x_2 - x_1}\] <p>As we have seen above we can pick any \(x_2\) that is close enough to \(x_1\) to get our slope so we can formalise that with any constant \(x_2 = x_1 + h\) hence getting:</p> \[\frac {f(x_1 + h) - f(x_1)} {x_1 + h - x_1} = \frac {f(x + h) - f(x)} { h }\] <p>If we want to get a point very close to <code class="language-plaintext highlighter-rouge">x + h</code> we can look at the limit of <code class="language-plaintext highlighter-rouge">h</code> towards <code class="language-plaintext highlighter-rouge">0</code>:</p> \[\lim _{h\to 0} \frac {f(x + h) - f(x)} { h } = \lim _{h\to 0} \frac {\Delta x} { \Delta h }\] <p>That approximation to a point is what differentiation is all about! Let’s try to resolve that limit for \(x^2\):</p> <p>\(\lim _{h\to 0} \frac {f(x + h) - f(x)} { h } =\) \(\lim _{h\to 0} \frac {(x + h)^2 - x^2} { h } =\) \(\lim _{h\to 0} \frac {x^2 + h^2 + 2xh - x^2} { h } =\) \(\lim _{h\to 0} \frac {h^2 + 2xh} { h } =\) \(\lim _{h\to 0} \frac {h(h+2x)} { h } =\) \(\lim _{h\to 0} h+2x = 2x\)</p> <p>We just derived \(x^2\)! If you recall <a href="https://en.wikipedia.org/wiki/Power_rule">the power rule</a> it tells us that given \(f(x) = x^n\) it’s derivative is:</p> <ul> <li>Prime notation: \(f'(x) = nx^{n-1}\)</li> <li>Leibniz notation: \(\frac d {dx} f(x) = nx^{n-1}\)</li> <li>Euler notation: \(\frac \partial {\partial x} f(x) = nx^{n-1}\)</li> </ul> <p><a href="https://en.wikipedia.org/wiki/Differentiation_rules">There are more rules</a> for differentiation which will allow us to get the <em>slope</em> (or <em>rate of change</em>) for any differentiable function (continuous, smooth and without tangents).</p> <p>Actually the slope we got earlier by approximating <code class="language-plaintext highlighter-rouge">x=2</code> was <code class="language-plaintext highlighter-rouge">4</code>, i.e. \(slope = f'(x^2) = 2x = 2*2 = 4\).</p> <h3 id="impact-of-changing-each-parameter">Impact of changing each parameter</h3> <p>At this point, we have defined our problem statement, we are able to measure its’ error for certain parameters and we are able to see how much a change in each parameter impacts that error by using the derivatives of each parameter (or <em>gradient</em>)! As a reminder, the function we are trying to minimise as described above is the <code class="language-plaintext highlighter-rouge">MAE</code>:</p> \[MAE = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>We will start by looking at what’s the impact of each change in <code class="language-plaintext highlighter-rouge">a</code> to that function (so that we can find an <code class="language-plaintext highlighter-rouge">a</code> that minimises its value) by differentiation. Our function is composite, since <code class="language-plaintext highlighter-rouge">MAE</code> depends on the output of our quadratic function. We need to apply <a href="https://en.wikipedia.org/wiki/Chain_rule">the Chain Rule</a> to be able to take the derivative, which states:</p> \[h'(x)=f'(g(x))g'(x)\] <p>Applied to our case, the inner function <code class="language-plaintext highlighter-rouge">g(x)</code> is:</p> \[g(x) = \hat y_i = ax_i^2 + bx_i + c\] <p>and the outer function <code class="language-plaintext highlighter-rouge">f(x)</code> is:</p> \[f(x) = \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i|\] <p>Resolving the derivative of our <code class="language-plaintext highlighter-rouge">MAE</code> to find out the impact of <code class="language-plaintext highlighter-rouge">a</code> through the chain rule, we get:</p> <p>\(\frac \partial {\partial a} MAE = \frac \partial {\partial a} \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i| =\) \(\frac \partial {\partial {\hat y_i}} \frac 1 n \sum _{i=1} ^n |y_i - \hat y_i| * { \frac \partial {\partial a} \hat y_i } =\) \(\frac 1 n \sum _{i=1} ^n \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| * { \frac \partial {\partial a} \hat y_i }\)</p> <p>Now we have two derivatives to resolve, let’s do that one by one:</p> <ol> <li> \[{ \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| }\] </li> <li> \[{ \frac \partial {\partial a} \hat y_i }\] </li> </ol> <p>Earlier we mentioned that one problem with <code class="language-plaintext highlighter-rouge">MAE</code> is that it’s not differentiable, and that’s because of the shape of the absolute function <code class="language-plaintext highlighter-rouge">|x|</code>:</p> <pre><code class="language-chartjs">{
  "type": "line",
  "data": {
    "labels": [-1,0, 1],
    "datasets": [
      {
        "label": "x",
        "fill": false,
        "lineTension": 0.1,
        "backgroundColor": "rgba(75,192,192,0.4)",
        "borderColor": "rgba(75,192,192,1)",
        "borderCapStyle": "butt",
        "borderDash": [],
        "borderDashOffset": 0,
        "borderJoinStyle": "miter",
        "pointBorderColor": "rgba(75,192,192,1)",
        "pointBackgroundColor": "#fff",
        "pointBorderWidth": 5,
        "pointHoverRadius": 10,
        "pointHoverBackgroundColor": "rgba(75,192,192,1)",
        "pointHoverBorderColor": "rgba(220,220,220,1)",
        "pointHoverBorderWidth": 2,
        "pointRadius": 1,
        "pointHitRadius": 10,
        "data": [1, 0, 1],
        "spanGaps": false
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "display": true,
        "title": {
          "display": true,
          "text": "x"
        }
      },
      "y": {
        "display": true,
        "title": {
          "display": true,
          "text": "y"
        }
      }
    }
  }
}
</code></pre> <p>Whenever <code class="language-plaintext highlighter-rouge">x=0</code> (\(\hat y_i - y_i = 0\)), we don’t know what’s the slope of the line! Looking from the left its negative and looking from the right it’s positive. We need to rely on a <a href="https://en.wikipedia.org/wiki/Subderivative">subdifferentials</a> for this function. For <code class="language-plaintext highlighter-rouge">x=0</code> it gives us a range of <code class="language-plaintext highlighter-rouge">[-1, 1]</code>. By convention we will pick the number <code class="language-plaintext highlighter-rouge">y=0</code> when <code class="language-plaintext highlighter-rouge">x=0</code>. The other particularities of the absolute function is that it converts all numbers to positive, i.e. <code class="language-plaintext highlighter-rouge">if x&gt;0 y=x</code> and <code class="language-plaintext highlighter-rouge">if x&lt;0 y=-x</code>.</p> <p>Now let’s apply the derivative knowing that:</p> <p>\({ \frac \partial {\partial {\hat y_i}} |y_i - \hat y_i| } = {\begin{cases} \frac \partial {\partial {\hat y_i}} (-y_i + \hat y_i) &amp; {y_i - \hat y_i} &lt; 0 \\ \frac \partial {\partial {\hat y_i}} (y_i - \hat y_i) &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}} =\) \({\begin{cases} 1 &amp; {y_i - \hat y_i} &lt; 0 \\ -1 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>The other derivative is just a simple application of the Power Rule:</p> <p>\(\frac \partial {\partial a} \hat y_i =\) \(\frac \partial {\partial a} ax^2 + bx + c = x^2\)</p> <p>And we got our <code class="language-plaintext highlighter-rouge">Quadratic MAE</code> full derivative with respect to <code class="language-plaintext highlighter-rouge">a</code>:</p> <p>\(\frac \partial {\partial a} MAE =\) \({\begin{cases} x^2 &amp; {y_i - \hat y_i} &lt; 0 \\ -x^2 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>Repeating the exact same process for <code class="language-plaintext highlighter-rouge">b</code> the only change is the <a href="https://en.wikipedia.org/wiki/Partial_derivative">partial derivative</a> of our quadratic:</p> <p>\(\frac \partial {\partial b} MAE =\) \({\begin{cases} x &amp; {y_i - \hat y_i} &lt; 0 \\ -x &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>and the same for <code class="language-plaintext highlighter-rouge">c</code>:</p> <p>\(\frac \partial {\partial c} MAE =\) \({\begin{cases} 1 &amp; {y_i - \hat y_i} &lt; 0 \\ -1 &amp; {y_i - \hat y_i} &gt; 0 \\ undefined &amp; {y_i - \hat y_i} = 0 \end{cases}}\)</p> <p>Now we have a way to calculate how much of an impact a unit change on <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> or <code class="language-plaintext highlighter-rouge">c</code> has on our error function with our quadratic. That vector is our <em>gradient</em>.</p> <h3 id="gradient-descent">Gradient descent</h3> <p>We are all set to find a solution to our original problem statement through Gradient Descent. There is <a href="https://en.wikipedia.org/wiki/Gradient_descent#An_analogy_for_understanding_gradient_descent">a nice analogy in Wikipedia</a> to help grasp Gradient Descent: a person is in a mountain and wants to go down. There is plenty of fog so the easiest way down is by looking at the slope and going one step at a time downwards all the way until the slope is 0.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gradient-descent-480.webp 480w,/assets/img/gradient-descent-800.webp 800w,/assets/img/gradient-descent-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/gradient-descent.jpg" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The slope in our case is the direction to minimise our error and gradient descent would look like:</p> <ul> <li>Start with a random values for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> coefficients (step 0)</li> <li>Get the slopes for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code></li> <li>Update <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> in the opposite direction of the gradient (since we want to decrease the error) by applying a learning rate to ensure we move in small steps (and not whole units)</li> <li>Repeat until we reduced the error rate enough or reach a slope of 0</li> </ul> <h3 id="a-sample-in-ruby">A sample in Ruby</h3> <p>Let’s see how the resolution of our original problem would look like in Ruby. First we need the slopes for <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">c</code> given \(x_i\), \(y_i\) and \(\hat y_i\):</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_mae_gradient_for_abc</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">)</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">length</span>

  <span class="n">quadratic_partials</span><span class="p">.</span><span class="nf">map</span> <span class="k">do</span> <span class="o">|</span><span class="n">partial_derivative</span><span class="o">|</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">).</span><span class="nf">map</span> <span class="k">do</span> <span class="o">|</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="o">|</span>
      <span class="n">abs_subgradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">partial_derivative</span><span class="p">.</span><span class="nf">call</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">end</span>

    <span class="n">gradients</span><span class="p">.</span><span class="nf">sum</span> <span class="o">/</span> <span class="n">n</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">abs_subgradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="p">)</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span> <span class="o">&lt;</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span> <span class="o">&gt;</span> <span class="mi">0</span>
  
  <span class="mi">0</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">quadratic_partials</span>
  <span class="n">quadratic_partial_a</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="p">}</span>
  <span class="n">quadratic_partial_b</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">x</span> <span class="p">}</span>
  <span class="n">quadratic_partial_c</span> <span class="o">=</span> <span class="o">-&gt;</span> <span class="n">_</span> <span class="p">{</span> <span class="mi">1</span> <span class="p">}</span>
  
  <span class="p">[</span><span class="n">quadratic_partial_a</span><span class="p">,</span> <span class="n">quadratic_partial_b</span><span class="p">,</span> <span class="n">quadratic_partial_c</span><span class="p">]</span>
<span class="k">end</span>
</code></pre></div></div> <p>Now that we can calculate the gradients we can go down the slope:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_gradient_descent</span><span class="p">(</span><span class="n">initial_parameters</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">)</span>
  <span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">initial_parameters</span>

  <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">..</span><span class="n">iterations</span> <span class="k">do</span>
    <span class="nb">print</span> <span class="s2">"</span><span class="se">\r</span><span class="s2">Running iterations.. (</span><span class="si">#{</span><span class="n">i</span><span class="si">}</span><span class="s2">) with learning rate </span><span class="si">#{</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">predicted_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">predicted_parameters</span><span class="p">)</span>
    <span class="n">predicted_y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">predicted_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}</span>

    <span class="n">gradients</span> <span class="o">=</span> <span class="n">calculate_mae_gradient_for_abc</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_y_i</span><span class="p">)</span>

    <span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">predicted_parameters</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">).</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">param</span><span class="p">,</span> <span class="n">slope</span><span class="o">|</span> <span class="n">param</span> <span class="o">-</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="p">}</span>
  <span class="k">end</span>

  <span class="n">predicted_parameters</span>
<span class="k">end</span>
</code></pre></div></div> <p>We use a small helper to create generic quadratic functions so that we can evaluate the predictions:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quadratic_from_parameters</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
  <span class="o">-&gt;</span> <span class="n">x</span> <span class="p">{</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span> <span class="p">}</span>
<span class="k">end</span>
</code></pre></div></div> <p>Now we can execute gradient descent on our problem! Let’s first see how we generate some fake sensor data, starting with our independent variable <code class="language-plaintext highlighter-rouge">x</code> (measured in seconds) that goes from 0 to 5 in 15 steps:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_x_i</span><span class="p">(</span><span class="n">from</span><span class="p">,</span> <span class="n">to</span><span class="p">,</span> <span class="n">number_of_steps</span><span class="p">)</span>
  <span class="n">step_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">to</span> <span class="o">-</span> <span class="n">from</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">number_of_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="nf">to_f</span>
  <span class="no">Array</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span><span class="n">number_of_steps</span><span class="p">)</span> <span class="p">{</span> <span class="o">|</span><span class="n">i</span><span class="o">|</span> <span class="n">from</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">step_size</span> <span class="p">}</span>
<span class="k">end</span>

<span class="n">x_i</span> <span class="o">=</span> <span class="n">generate_x_i</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</code></pre></div></div> <p>Next we are ready to generate values for <code class="language-plaintext highlighter-rouge">y</code>. We use a function that follows the equation of a vertically thrown ball introduced in the beginning of the article \(h(t)=−4.9t^2+v0​t+h0​\) with an initial speed of <code class="language-plaintext highlighter-rouge">v0=20</code> and a height of <code class="language-plaintext highlighter-rouge">h0=30</code>. We also add some noise to fake imperfect sensor data:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quadratic_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">-</span><span class="mf">4.9</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>

<span class="n">y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">quadratic_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}.</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">y</span><span class="o">|</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span><span class="o">*</span><span class="nb">rand</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="o">..</span><span class="mf">0.05</span><span class="p">)</span> <span class="p">}</span>
</code></pre></div></div> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">initial_random_abc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">521</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="o">-</span><span class="mi">120</span><span class="p">]</span>

<span class="n">predicted_parameters</span> <span class="o">=</span> <span class="n">run_gradient_descent</span><span class="p">(</span><span class="n">initial_random_abc</span><span class="p">,</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="nb">puts</span> <span class="s2">"Parameters prediction: </span><span class="si">#{</span><span class="n">predicted_parameters</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div></div> <p>and we get:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running iterations.. <span class="o">(</span>100000<span class="o">)</span> with learning rate 0.005
Parameters prediction: <span class="o">[</span><span class="nt">-5</span>.09634353758922, 20.90880952378543, 29.844999999961708]
</code></pre></div></div> <p>Our algorithm predicted the following equation:</p> \[f(x) = -5.09x^2 + 20.91x + 29.85\] <p>When compared to the ball-throwing equation we have on the top, we see how close we got:</p> <ul> <li>Predicted values: \(g = -10.18\), \(v0=20.91\) and \(h0=29.85\)</li> <li>Actual values we used to generate our (noisy) data: \(g = 9.8\), \(v0=20\) and \(h0=30\)</li> </ul> <p>You can easily calculate the MAE value if needed with:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mae</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_parameters</span><span class="p">)</span>
  <span class="n">predicted_equation</span> <span class="o">=</span> <span class="n">quadratic_from_parameters</span><span class="p">(</span><span class="o">*</span><span class="n">predicted_parameters</span><span class="p">)</span>
  <span class="n">predicted_y_i</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">.</span><span class="nf">map</span><span class="p">{</span> <span class="o">|</span><span class="n">x</span><span class="o">|</span> <span class="n">predicted_equation</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">}</span>
  <span class="n">differences</span> <span class="o">=</span> <span class="n">y_i</span><span class="p">.</span><span class="nf">zip</span><span class="p">(</span><span class="n">predicted_y_i</span><span class="p">).</span><span class="nf">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted_y</span><span class="o">|</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">predicted_y</span><span class="p">).</span><span class="nf">abs</span> <span class="p">}</span>
  <span class="n">differences</span><span class="p">.</span><span class="nf">sum</span> <span class="o">/</span> <span class="n">differences</span><span class="p">.</span><span class="nf">size</span><span class="p">.</span><span class="nf">to_f</span>
<span class="k">end</span>

<span class="nb">puts</span> <span class="s2">"MAE: </span><span class="si">#{</span><span class="n">mae</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">predicted_parameters</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div></div> <p>which for our sample run was <code class="language-plaintext highlighter-rouge">MAE: 1.025</code>.</p> <h2 id="conclusion">Conclusion</h2> <p>We have been able to use Gradient Descent to predict the coefficients of a quadratic function with only dirty sample data! We even almost got right the value of gravity! There is a big assumption we’ve done though: we knew our sensor data (throwing a ball vertically) followed a quadratic function.</p> <p>In a next post we will see how we can generalise what we learnt here to resolve more complicated problems like those mentioned in the beginning that don’t necessarily follow a quadratic function with the help of Rectified Linear Units.</p> <p><a href="https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work">Check this Jupyter notebook</a> for interactive learning material or <a href="https://www.youtube.com/watch?v=hBBOjCiFcuo">its’ associated video</a> to grasp better the concepts explained in this article.</p>]]></content><author><name></name></author><category term="exploration"/><category term="ai"/><summary type="html"><![CDATA[A deep-dive on how Gradient Descent works from the math behind it to a working Ruby sample.]]></summary></entry><entry><title type="html">AI Bias and Feedback loops</title><link href="https://www.gerovlabs.com/blog/ai-bias-and-feedback-loops/" rel="alternate" type="text/html" title="AI Bias and Feedback loops"/><published>2024-10-06T00:00:00+00:00</published><updated>2024-10-06T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/ai-bias-and-feedback-loops</id><content type="html" xml:base="https://www.gerovlabs.com/blog/ai-bias-and-feedback-loops/"><![CDATA[<p>As usage of social networks keeps growing so does the power of their underlying algorithms with their ever-growing datasets and AI capabilities. They are optimised for interactions and/or time-spent on the platforms while only becoming better through time. No ethical lenses seem to be baked into such systems since their main incentive is economical growth. <a href="https://www.reuters.com/article/world/un-investigators-cite-facebook-role-in-myanmar-crisis-idUSKCN1GO2Q4/">Policy isn’t doing enough</a> to protect us so far.</p> <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/statista-social-usage-480.webp 480w,/assets/img/statista-social-usage-800.webp 800w,/assets/img/statista-social-usage-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/statista-social-usage.png" class="img-fluid rounded z-depth-1 w-50 " width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Daily time spent on social media 2012-2024 according to <a href="https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/">Statista</a> </div> </div> <p>The issue goes way beyond choosing the right metric to optimise for since bias and hazardous feedback loops are embedded in the whole delivery chain. The Markulla Center <a href="https://www.scu.edu/ethics/ethics-resources/a-framework-for-ethical-decision-making/">defines a set of ethical lenses</a> which can help us navigate and understand the problems imposed in the current tech landscape:</p> <ul> <li><code class="language-plaintext highlighter-rouge">Rights</code>: focuses on respecting the fundamental rights of all individuals involved. It prompts us to ask if a particular action or tech infringes on anyone’s rights such as privacy, safety, or equal opportunity</li> <li><code class="language-plaintext highlighter-rouge">Justice</code>: aims fairness and equity. It encourages us to consider whether an action or tech treats people fairly and whether it perpetuates existing inequalities</li> <li><code class="language-plaintext highlighter-rouge">Utilitarian</code>: involves assessing the overall benefits/harms of an action or tech. The goal is to pick the option that maximises benefit and minimises harm for the greatest number of people possible</li> <li><code class="language-plaintext highlighter-rouge">Common Good</code>: emphasises the well-being of the entire community as a whole, not just specific individuals or subgroups. It prompts us to consider how an action or tech affects the overall health and society</li> <li><code class="language-plaintext highlighter-rouge">Virtue</code>: considers cultivating ethical character and aligning actions with virtues (like honesty, responsibility, and compassion). It encourages individuals and organisations to act reflecting their values</li> </ul> <p>Let’s have a look at some relevant examples we can look at through these lenses:</p> <p><strong>YouTube recommendations engine</strong></p> <p>The reinforcement learning feedback loops optimised for maximising watch time can be devastating. It can happen by accident <a href="https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html">by promoting pedophile content</a> or <a href="https://www.nytimes.com/2017/10/23/technology/youtube-russia-rt.html">on purpose by exploiting algorithms nature</a>.</p> <p>Such examples violate children rights to safety and privacy or individuals rights to access accurate information by promoting conspiracy theories. Also by disproportionately recommending extremist content, YouTube’s algorithm creates an uneven playing field where certain viewpoints, often harmful and misleading, gain prominence.</p> <p><strong>Google ads race discrimination</strong></p> <p><a href="https://arxiv.org/abs/1301.6822">A study</a> conducted by Professor Latanya Sweeney revealed that Google displayed ads suggesting an arrest record more frequently for searches of black-identifying names than for white-identifying names, regardless of whether the individuals had any arrest history or not.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ads-sample-480.webp 480w,/assets/img/ads-sample-800.webp 800w,/assets/img/ads-sample-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ads-sample.png" class="img-fluid rounded z-depth-1 w-md-50 float-md-right ml-md-2" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It supposes a violation of individuals rights to equal treatment and non-discrimination by perpetuating harmful stereotypes and creating an environment where individuals are treated differently based on their race leading to unequal opportunities.</p> <p><strong>Arkansas healthcare</strong></p> <p>A buggy and opaque algorithm <a href="https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy">led to drastic cuts in healthcare benefits</a> for vulnerable individuals which represents a failure on fundamental rights to essential healthcare services. It exemplifies an unjust and disproportionate impact of algorithmic errors on marginalised communities.</p> <p><strong>Amplifying Gender Imbalance</strong></p> <p><a href="https://arxiv.org/abs/1901.09451">Researchers found</a> algorithms predicting occupations not only reflected existing gender imbalances in occupations (e.g. more female nurses, more male pastors) but actually amplified them in its predictions. The model oversimplified the relationship between gender and occupation, leading to an overrepresentation of the dominant gender in certain professions, thus perpetuating existing inequalities.</p> <h3 id="conclusion">Conclusion</h3> <p>These examples showcase the urgent need for ethical considerations while developing, deploying, and regulating AI and algorithmic systems. They highlight the potential for these technologies to amplify existing societal biases and cause real-world harm, specially to vulnerable populations. Addressing the challenges requires a cross-disciplinary approach including tech, regulation, and a strong ethical guidance for developers, policymakers, and end users.</p> <p>The advent of LLMs to generate human-quality text at scale further compounds the issue since exploiting existing systems, biases and human flaws become even more prominent.</p> <p>For further information please read <a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Chapter 3 of fast.ai book</a>.</p>]]></content><author><name></name></author><category term="exploration"/><category term="ai"/><summary type="html"><![CDATA[On how AI imposes a danger and further perpetuates human flaws.]]></summary></entry><entry><title type="html">Deploying a fine-tuned classifier</title><link href="https://www.gerovlabs.com/blog/fastai-finetuning-a-classifier/" rel="alternate" type="text/html" title="Deploying a fine-tuned classifier"/><published>2024-09-29T00:00:00+00:00</published><updated>2024-09-29T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/fastai-finetuning-a-classifier</id><content type="html" xml:base="https://www.gerovlabs.com/blog/fastai-finetuning-a-classifier/"><![CDATA[<p>Two chapters into the <a href="https://course.fast.ai/">fast.ai course</a> and we are already equipped to deploy a fine-tuned classifier that operates on a close-to-human level task without too much data nor expensive hardware (actually free)!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lotr-races-2-480.webp 480w,/assets/img/lotr-races-2-800.webp 800w,/assets/img/lotr-races-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/lotr-races-2.png" class="img-fluid rounded z-depth-1 w-md-50 float-md-right ml-md-2" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As a pet project I ended up building <strong><a href="https://gerovlabs.com/ai-models/">a LOTR race classifier</a></strong>. It achieves quite good accuracy (<strong>75%</strong> on my validation set, I felt it did even better during QA) given how much I’m relying on defaults and how little (and dirty) the custom input dataset is.</p> <p>The process starts with data gathering (using <a href="https://serpapi.com/duckduckgo-search-api">DDG API</a> and some helpers from fast.ai) with ~100 images of each LOTR race. Afterwards a <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html">resnet18</a> model is fine-tuned so that we can check the images that perform the worst and use them to clean up the data further. Once we are happy with the dataset we re-train the model and do the transformations we see fit. Once the fine-tuned model is ready we export and deploy it to <a href="https://huggingface.co/spaces">HG Spaces</a>. Afterwards we went for a custom solution so we could see how an E2E deployment would look like.</p> <pre><code class="language-typograms">+-------------+    +-------+    +------------+    +--------+ 
| Gather data |---&gt;| Train |---&gt;| Clean data |---&gt;| Deploy |
+-------------+    +-------+    +------------+    +--------+ 
                        ^            |
                        +------------+
                           re-train
</code></pre> <h3 id="data">Data</h3> <p>I’ve had data scientist colleagues mention to me how much of their time went into getting clean data (or structuring corporate data, <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">up to 80%</a>). I’m not working on a production system and still found it tricky to get good-enough data even for my small experiment.</p> <p>A simplified version of how I gathered the data looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">race_characters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">Ainur</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Gandalf</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Saruman</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Sauron</span><span class="sh">'</span><span class="p">],</span> <span class="c1"># etc..
</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">fetch_images_for_race</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">race</span><span class="p">,</span> <span class="n">characters</span> <span class="ow">in</span> <span class="n">race_characters</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">folder</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">./</span><span class="si">{</span><span class="n">race</span><span class="si">}</span><span class="sh">"</span>
        <span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="nc">DDGS</span><span class="p">().</span><span class="nf">images</span><span class="p">(</span>
                <span class="n">keywords</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">character</span><span class="si">}</span><span class="s"> lotr movie face</span><span class="sh">"</span> <span class="k">if</span> <span class="n">race</span> <span class="o">!=</span> <span class="sh">'</span><span class="s">no-middle-earth</span><span class="sh">'</span> <span class="k">else</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">character</span><span class="si">}</span><span class="s"> face</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">type_image</span><span class="o">=</span><span class="sh">"</span><span class="s">photo</span><span class="sh">"</span><span class="p">,</span>
                <span class="n">max_results</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="nf">download_images</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">urls</span><span class="o">=</span><span class="n">results</span><span class="p">)</span> <span class="c1"># this is a fast.ai utility function
</span></code></pre></div></div> <h3 id="first-model">First model</h3> <p>Training and cleaning the dataset is well covered in <a href="https://course.fast.ai/Lessons/lesson2.html">the second chapter</a> of the fast.ai course although I’ll share some issues encountered on the way:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/after-cleaning-480.webp 480w,/assets/img/after-cleaning-800.webp 800w,/assets/img/after-cleaning-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/after-cleaning.png" class="img-fluid rounded float-right mx-2 w-50" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><code class="language-plaintext highlighter-rouge">Duplicate images</code>: I realised quite a few of the images I was downloading were duplicates</li> <li><code class="language-plaintext highlighter-rouge">Using a non-structured dataset</code>: searching the web for images implies a huge bias on what kind of results you would get and how accurate it is. I had to adjust my queries multiple times until I liked enough the shape of the data.</li> <li><code class="language-plaintext highlighter-rouge">Cleaning data</code>: fast.ai provides a very limited utility to clean-up your training/validation image datasets. It’s not optimised for quick cleanup and is lacking essential features to enable a quicker process. I feel that at this stage looking manually at the pictures and/or relying on other automated software would have been quicker.</li> <li><code class="language-plaintext highlighter-rouge">Remote environment</code>: using <a href="https://kaggle.com/">Kaggle</a> is a huge enabler for me since my Mac doesn’t come with a GPU but I find it more comfortable to run most of my code locally and only rely on it when I get to using the GPU. That becomes a problem when some operations (like cleaning up the data) depend on previously training your model.</li> <li><code class="language-plaintext highlighter-rouge">Kaggle notebook</code>: fast.ai chapters notebooks were done a while ago and some packages like DDG have evolved and the actual Kaggle environment has changed so there was some manual intervention needed in the actual code or setup. That’s OK for tech-savvy people but could be a blocker for people with less ops experience.</li> <li><code class="language-plaintext highlighter-rouge">Deployment</code>: deploying to <a href="https://huggingface.co/spaces/sgerov/lotr-races">a personal Space in Hugging Face</a> worked as well but I also had to adjust the Gradio sample code I read. Same happened for some configurations I had to add for HG deployments to work.</li> </ul> <h3 id="custom-solution">Custom solution</h3> <p>The <a href="https://huggingface.co/spaces/sgerov/lotr-races">Gradio &amp; Hugging Face Space</a> deployment worked well but I wanted to have a custom deployment to get an E2E feeling of a simple release.</p> <p>I already had a working <a href="https://jekyllrb.com/">Jekyll</a> deployment (this site) so I decided to include a <a href="https://react.dev/">React</a> application inside where I could communicate with a Backend for inference.</p> <p>I started trying to integrate the HF Space JS client into React but sadly <a href="https://github.com/gradio-app/gradio/issues/7693">it didn’t work</a>. The prediction code would be as simple as:</p> <div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">fetch</span><span class="p">(</span><span class="dl">"</span><span class="s2">https://raw.githubusercontent.com/gradio-app/gradio/main/test/test_files/bus.png</span><span class="dl">"</span><span class="p">);</span>
<span class="kd">const</span> <span class="nx">exampleImage</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">response</span><span class="p">.</span><span class="nf">blob</span><span class="p">();</span>
						
<span class="kd">const</span> <span class="nx">client</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">Client</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="dl">"</span><span class="s2">sgerov/lotr-races</span><span class="dl">"</span><span class="p">);</span>
<span class="kd">const</span> <span class="nx">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">client</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="dl">"</span><span class="s2">/predict</span><span class="dl">"</span><span class="p">,</span> <span class="p">{</span> 
				<span class="na">img</span><span class="p">:</span> <span class="nx">exampleImage</span><span class="p">,</span> 
<span class="p">});</span>
</code></pre></div></div> <p>Next I went for <a href="https://www.gradio.app/guides/querying-gradio-apps-with-curl">the manual endpoint integration</a> but I wasn’t getting the right events from the <code class="language-plaintext highlighter-rouge">$URL/call/$API_NAME/$EVENT_ID</code> regardless of how I formatted the request.</p> <p>This is the point where I decided to go for a custom solution which is something I wanted to try out anyway. I went for the least resistance path that came to mind which was to rely on a very slim <a href="https://www.djangoproject.com/">Django app</a> that relies on <a href="https://docs.fast.ai/">fast.ai library</a> for inferences:</p> <pre><code class="language-typograms"> +--------+ html .---------. base64 image  +------------+    
 |        |-----&gt;|         |--------------&gt;| Django App |
 | Jekyll |      | Browser |&lt;--------------| .--------. |
 +--------+      +---------+    prediction | |fast.ai | |
     ^          / /       \ \              | .--------. |
     |         '-------------'             +------------+
     |JS                                         ^
     |                                           |model.pkl
+-----------+                            +-----------------+              
| React APP |                            | Kaggle training |               
+-----------+                            +-----------------+
</code></pre> <p>A great side-effect of a custom solution is that it was rather trivial to make a mobile-friendly version of the app which might enable some interesting model experiments later on.</p> <p>The Django endpoint, aside of the glueing code given by the framework is almost as simple as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pkl</span><span class="sh">'</span><span class="p">)</span>
<span class="n">image_base64</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">PILImage</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">file_from_image_base64</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">predictions</span><span class="sh">'</span><span class="p">:</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">dls</span><span class="p">.</span><span class="n">vocab</span><span class="p">,</span> <span class="nf">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">outputs</span><span class="p">))),</span>
<span class="p">}</span>
<span class="k">return</span> <span class="nc">JsonResponse</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <h3 id="next-steps">Next steps</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lotr-races-1-480.webp 480w,/assets/img/lotr-races-1-800.webp 800w,/assets/img/lotr-races-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/lotr-races-1.png" class="img-fluid rounded z-depth-1 float-right ml-2" width="150px" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now that there is some backbone and we got some experience deploying simple models we should be all set to experiment with fast.ai configurations, input data, different architectures and datasets while <a href="https://course.fast.ai/Lessons/lesson3.html">digging into deep learning</a>.</p> <p>Some immediately useful community contributions that come to mind are:</p> <ul> <li>Improve the <a href="https://docs.fast.ai/vision.widgets.html#imageclassifiercleaner">Image Classifier Cleaner</a> to add additional features that could speed up dramatically the data cleaning process</li> <li>Create a new widget to enable quicker feedback loops and labeling while downloading data from platforms like DDG</li> <li>Improve course notebooks (there are already open PRs handling this on fast.ai)</li> </ul> <h3 id="useful-links">Useful links</h3> <ul> <li><a href="https://gerovlabs.com/ai-models/">Custom LOTR race classifier</a></li> <li><a href="https://huggingface.co/spaces/sgerov/lotr-races">HG + Gradio deployment</a></li> <li><a href="https://www.kaggle.com/code/savagerov/training-a-model-lotr">Kaggle training notebook</a></li> <li><a href="https://github.com/sgerov/gerovlabs-models-serve">Django toy server</a></li> <li><a href="https://course.fast.ai/Lessons/lesson2.html">Second lesson of fast.ai course</a></li> <li><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Chapter 2 of fast.ai book</a></li> </ul>]]></content><author><name></name></author><category term="exploration"/><category term="ai"/><summary type="html"><![CDATA[Leveraging fast.ai capabilities for a quick resnet model fine-tuning and delivery.]]></summary></entry><entry><title type="html">Collaboration and communication</title><link href="https://www.gerovlabs.com/blog/collaboration-and-communication/" rel="alternate" type="text/html" title="Collaboration and communication"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/collaboration-and-communication</id><content type="html" xml:base="https://www.gerovlabs.com/blog/collaboration-and-communication/"><![CDATA[<p>Regardless of where we are in a project’s timeline, effective collaboration and communication are key not only for its success but also for <strong>managing expectations</strong> and <strong>ensuring alignment</strong> across and within teams.</p> <p>Effective collaboration and communication involves many aspects, from structured O3s (1-1s) and sharp facilitation skills, to timely escalation and generating a safe space for <strong>open dialogue</strong>. It’s about more than just talking - it’s about getting the right people involved in the right conversations, setting clear role expectations, and ensuring that everyone is on the same page.</p> <blockquote class="block-tip"> <h5 id="example">Example</h5> <p>Architectural Decision Records (ADRs) provide clarity and accountability, while proactive feedback loops and/or regular retrospectives ensure issues are surfaced and addressed early.</p> </blockquote> <p>In addition, understanding team dynamics, showing empathy, and maintaining transparency can drastically <strong>improve collaboration</strong>. By fostering <strong>psychological safety</strong>, team members are encouraged to share their ideas and concerns freely, creating a culture where everyone feels heard and respected. Trust is built not just through communication, but through actions, follow-through, and mutual accountability.</p> <p>Whether it’s refining written communication skills, <strong>ensuring clarity</strong> in decision-making, or creating a culture of <strong>continuous feedback</strong>, effective collaboration goes beyond processes! It’s about fostering strong, resilient <strong>relationships</strong> that drive the project forward.</p> <p>Fore more on effective collaboration, please check the <a href="/blog/integrity-and-transparency/">Integrity and Transparency</a> article.</p>]]></content><author><name></name></author><category term="core-values"/><summary type="html"><![CDATA[Managing expectations and ensuring alignment across and within teams.]]></summary></entry><entry><title type="html">Delivering value over process</title><link href="https://www.gerovlabs.com/blog/deliver-value/" rel="alternate" type="text/html" title="Delivering value over process"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/deliver-value</id><content type="html" xml:base="https://www.gerovlabs.com/blog/deliver-value/"><![CDATA[<p>Processes are everywhere! Scrum, Kanban, Scrumban, no-estimates, PRs - the options are endless. And while processes provide structure and discipline, the real goal of any software project should be rather simple: <strong>delivering value</strong>.</p> <p>I believe that <strong>value delivery trumps rigid adherence to process</strong>. While I respect and utilize heavily established methodologies by the industry, I understand that clients don’t hire us for the method but for the outcome.</p> <h4 id="why-value-first">Why “Value First”</h4> <p>The danger of focusing too much on process (or specific metrics) is that it can become an end in itself. Teams can find themselves bogged down by rituals, meetings, and a strict sequence of steps. Before you know it, the focus has shifted from solving the real problem to either making sure every process box is checked or metric X is green.</p> <p>Is a flawlessly executed sprint really valuable if the needs haven’t been fully addressed? Or if the product doesn’t solve the core problem?</p> <p>The philosophy is simple: <strong>processes are tools, not rules</strong>. They are there to help us, not hinder us. If we’re following a process and it’s no longer serving the project’s real goal (i.e. delivering business value) we adapt, pivot, and change course. At the end of the day, what we care about most is the outcome.</p> <blockquote class="block-tip"> <h5 id="example">Example</h5> <p>The OKRs strive for setting objectives and key results that are outcome oriented and interconnected so that teams have the freedom to explore the solution space while focusing on moving the needle.</p> </blockquote> <h4 id="solving-the-right-problems">Solving the right problems</h4> <p>Delivering value means understanding the client’s core business needs. It means <strong>prioritizing functionality that moves the needle</strong>, whether that’s improving customer experience, reducing operational costs, or creating new revenue streams.</p> <p>Let’s start with asking the right (and sometimes uncomfortable) questions:</p> <ul> <li>What is the main business challenge we’re facing?</li> <li>How will success be measured in our project? (outcome vs output)</li> <li>What’s the most critical pain point for our users?</li> </ul> <p>These questions drive our development decisions. By focusing on the ultimate objectives, we ensure that every line of code we write and every feature we deliver contributes directly to meaningful outcomes.</p> <h4 id="flexibility-is-essential">Flexibility is essential</h4> <p>One of the biggest advantages of prioritizing value over process is <strong>flexibility</strong>. We believe in using <strong>Agile principles</strong> to stay adaptable, allowing us to shift gears when necessary. If client priorities change mid-project (and they often do), we adjust the plan accordingly, without getting derailed by rigid adherence to the original process.</p> <p>In a world that changes rapidly, this ability to adapt is crucial to delivering true customer value. I don’t believe in being tied to a strict playbook, nor that it’s the intention behind the quite misused Agile term.</p> <h4 id="client-satisfaction">Client satisfaction</h4> <p>At the end of a project, success isn’t measured by how well we followed a process. It’s measured by the impact we’ve made on the business. Did we help reach the goals? Did we solve the problems? Did we make the operations smoother or customers happier?</p> <p>That’s what delivering value is all about. It’s not about hitting sprint deadlines, following every Agile ceremony or achieving outputs over outcomes. It’s about knowing that the product we’ve delivered <strong>makes a real difference</strong> to the client’s bottom line.</p> <p>So, if you’re looking for a partner who understands that <strong>the end result</strong> is what really counts, let’s talk!</p>]]></content><author><name></name></author><category term="core-values"/><summary type="html"><![CDATA[The real goal of any software project should be rather simple - delivering value.]]></summary></entry><entry><title type="html">Ethical responsibility</title><link href="https://www.gerovlabs.com/blog/ethical-responsibility/" rel="alternate" type="text/html" title="Ethical responsibility"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/ethical-responsibility</id><content type="html" xml:base="https://www.gerovlabs.com/blog/ethical-responsibility/"><![CDATA[<p>As technologists, we have an immense amount of influence in todays’ world. The systems we design and the software we create shape industries, impact individuals, and define modern society. With this influence comes a significant responsibility: a responsibility not just to our clients or employers, but to society.</p> <p>At the heart of this responsibility is <strong>ethics</strong>. Ethical responsibility in software development is about more than writing clean code or delivering features on time. It’s about considering the broader implications of our work, ensuring that the products we build contribute positively to the world and avoid harm wherever possible.</p> <h3 id="power-and-pitfalls">Power and pitfalls</h3> <p>Software is a powerful tool. It can solve complex problems, improve efficiency, and create opportunities for innovation. But that same power can also be misused or misdirected, often unintentionally. We’ve seen countless examples of technology that was developed with good intentions but ended up causing significant harm: whether it’s biased algorithms, privacy breaches, or even systems that perpetuate inequality.</p> <blockquote class="block-tip"> <h5 id="example">Example</h5> <p><a href="https://www.theguardian.com/technology/commentisfree/2020/oct/15/cambridge-analytica-threat-democracy-facebook-big-tech">Camgridge Analytica</a> case is a good example of a product usage that started as a way to connect peers in Universities and escalated to shaping our democracies.</p> </blockquote> <p>This is why <strong>ethical reflection</strong> is essential at every stage of the software development process. When we build software, we aren’t just delivering code. We are embedding assumptions, values, and biases into systems. It’s our duty to ensure that these systems operate in a way that is <strong>fair, just, and responsible</strong>.</p> <h4 id="privacy-and-data">Privacy and data</h4> <p>One of the most critical ethical concerns in software development today is the issue of <strong>data privacy</strong>. In an age where data is the new currency, it’s tempting to collect as much information as possible. But just because we <strong>can</strong> collect data doesn’t mean we <strong>should</strong>.</p> <p>We also have a responsibility to ensure that the data we use to train models or make decisions is free from bias. <strong>Rebecca Parsons</strong> often speaks about the dangers of <strong>algorithmic bias</strong>: how seemingly neutral algorithms can perpetuate existing inequalities if they are trained on biased data. As developers, it’s our ethical responsibility to challenge these biases and ensure that the systems we create are fair and inclusive. We also need to ensure that our systems are <strong>auditable</strong> and <strong>explainable</strong>, users have the right to know how decisions that affect their lives are being made.</p> <p>As software professionals, it’s our ethical duty to recognize these risks and <strong>actively mitigate them</strong>. That means testing our systems rigorously, questioning our assumptions, and ensuring that the products we build work equally well for everyone: regardless of their race, gender, or socioeconomic background.</p> <h4 id="ethics-as-a-design-principle">Ethics as a design principle</h4> <p>One of the dangerous mindsets in development is the belief that <strong>“we’re just building the tool”</strong> and that what happens with it isn’t our responsibility. This abdication of ethical responsibility is rather short-sighted.</p> <p><strong>Ethical responsibility</strong> needs to be baked into the design process from day one. It’s not something that can be tacked on at the end, nor is it something that can be left to chance. We need to build ethics into the very fabric of our systems.</p> <p>That implies having tough conversations about the ethical implications of our decisions. It implies working with diverse teams to ensure that we’re considering multiple perspectives. It implies to be willing to slow down and reconsider our approach if we realize that what we’re building might cause harm.</p> <p>Ethical responsibility is not a burden. It’s a <strong>core principle</strong> of software development. When we take the time to build ethically, we build systems that are <strong>trustworthy, sustainable</strong>, and beneficial to society.</p>]]></content><author><name></name></author><category term="core-values"/><summary type="html"><![CDATA[The systems we design and the software we create shape industries, impact individuals, and define modern society.]]></summary></entry><entry><title type="html">Integrity and transparency</title><link href="https://www.gerovlabs.com/blog/integrity-and-transparency/" rel="alternate" type="text/html" title="Integrity and transparency"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/integrity-and-transparency</id><content type="html" xml:base="https://www.gerovlabs.com/blog/integrity-and-transparency/"><![CDATA[<p>In my years working with teams, both large and small, I’ve seen projects do better or worst and a vital factor was always at play: <strong>trust</strong>. Trust is what holds a team together, enables productive collaboration, and, ultimately, delivers successful outcomes. But trust doesn’t appear by chance! It’s earned, day by day, through the values of <strong>integrity and transparency</strong>. Let’s break that down:</p> <p>What does <strong>integrity</strong> mean in the context of software development? It means standing by your principles, doing what’s right even when it’s uncomfortable. It’s about owning up to mistakes and raising concerns early, not waiting for the cracks to become too wide to ignore.</p> <p><strong>Transparency</strong> goes hand-in-hand with integrity. It’s about ensuring that everyone on the team - whether it’s developers, stakeholders, or clients - has visibility into what’s really happening. Transparency requires openness, and more importantly, honesty.</p> <h4 id="speaking-up-timely">Speaking up timely</h4> <p>One of the biggest risks to any project is when people stay silent. A developer spots a potential problem - technical debt building up, a misaligned architectural decision, or a requirement that doesn’t seem right. But they don’t say anything. Maybe they’re afraid of causing a stir or being seen as negative. But by the time the problem comes to light, it might be too late.</p> <p>This is where <strong>a safe space</strong> is critical. It’s our responsibility to speak up when we see potential issues, and to do it <strong>early</strong>. Raising flags before they become fires may not be glamorous, but it saves time and stress. It’s better to ask uncomfortable questions now than to scramble to fix unspoken problems later. In fact, one of the best things we can do as leaders or team members is to <strong>normalize speaking up early</strong>.</p> <h4 id="the-courage-to-be-honest">The courage to be honest</h4> <p>Some of the most successful teams I’ve worked with were full of disagreements. The difference? They disagreed <strong>honestly</strong> and <strong>constructively</strong>.</p> <p>There’s a section in Brené Brown’s <em>Dare to Lead</em> that speaks to this, and it’s something I’ve found critical in software development: <strong>clear is kind</strong>. Being clear and honest with your team. Whether that’s admitting you’re unsure, explaining a risk, or acknowledging a mistake is one of the kindest and most productive things you can do!</p> <p>Honesty isn’t about pushing your opinion or being brutally blunt. It’s about presenting the facts and asking tough questions in the spirit of improving the project and ourselves. It’s about recognizing that <strong>our ego shouldn’t get in the way of transparency</strong>. Real courage comes from <strong>humility</strong>, from being willing to say, “I think I might be wrong” or “We need to reconsider this”.</p> <h4 id="transparency-builds-trust">Transparency builds trust</h4> <p>We often think of transparency in terms of communication between team members. But in my experience, <strong>transparency with stakeholders and clients</strong> is just as critical or more. Whether we’re in leadership roles or contributing as developers, we owe it to the people we work with to give an honest view of the status quo.</p> <p>Sometimes that means admitting we’re having issues. Sometimes it means admitting we don’t have a perfect solution yet. But in all cases, it means being honest about the state of the project and the challenges we face.</p> <p>Clients and stakeholders can handle bad news. What they can’t handle is being blindsided by them. If you’re open and transparent from the start - giving an accurate picture of progress, risks, and the potential roadblocks - you build <strong>trust</strong>. Even when things go wrong, that trust allows you to work together to find solutions. That’s tightly related to my understanding of how <strong>Agile principles</strong> are applied in practice.</p> <p>Humility is a strength. It shows that you’re focused on the <strong>success of the project</strong>, not on being right. When teams embrace humility, they create an environment where people are comfortable being honest. And when people are honest, the team becomes <strong>stronger, more agile, and more resilient</strong>.</p> <h4 id="integrity-as-a-competitive-advantage">Integrity as a competitive advantage</h4> <p>Some might think that this level of openness could be risky in a competitive world. But I’d argue that <strong>integrity and transparency are competitive advantages</strong>. By being upfront about risks, challenges, and solutions, you earn the trust of clients, stakeholders, and your team. That trust creates long-lasting partnerships, enables smoother project delivery, and most importantly: ensures that everyone is working together towards a common goal.</p> <p>In the end, software isn’t just about working code but about <strong>people</strong>. And the best way to work with people is with honesty, integrity, and transparency.</p>]]></content><author><name></name></author><category term="core-values"/><summary type="html"><![CDATA[By being upfront about risks, challenges, and solutions we earn the trust of clients, stakeholders, and team.]]></summary></entry><entry><title type="html">Simplicity and pragmatism</title><link href="https://www.gerovlabs.com/blog/simplicity-and-pragmatism/" rel="alternate" type="text/html" title="Simplicity and pragmatism"/><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://www.gerovlabs.com/blog/simplicity-and-pragmatism</id><content type="html" xml:base="https://www.gerovlabs.com/blog/simplicity-and-pragmatism/"><![CDATA[<p>In software development, there’s often a tendency to overcomplicate things (worstened by <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conways’ Law</a>). We’re constantly tempted to add more functionality, introduce more abstractions, or build complex architectures to cover every conceivable edge case. But the most effective and maintainable systems often share a common trait: <strong>simplicity</strong>.</p> <p>Simplicity is not about cutting corners or taking shortcuts. It’s about making decisions that reduce unnecessary complexity while still delivering value. In fact, simplicity is often harder to achieve than complexity. It takes discipline, experience, and a clear focus on the <strong>right</strong> problems to solve.</p> <p>At its core, <strong>simplicity is pragmatic</strong>. It’s about solving today’s problems, not tomorrow’s hypotheticals. It’s about avoiding over-engineering and focusing on what’s truly essential to deliver value. Let’s explore what that means in practice.</p> <blockquote class="block-tip"> <h5 id="example">Example</h5> <p>Premature optimisation is a classic example of overcomplicating things unnecessarily, seen commonly in grads coming from academia.</p> </blockquote> <h4 id="accidental-complexity">Accidental complexity</h4> <p>In my experience, most of the complexity in software systems is <strong>accidental</strong>, not <strong>essential</strong>. Accidental complexity comes from poor design choices, over-engineered solutions, and unnecessary abstractions (or architecture) that don’t solve the core problem but make the code harder to work with.</p> <p>As Sandi Metz often emphasizes in her work, the goal should always be to design systems that are <strong>flexible but simple</strong>. Too many devs fall into the trap of “future-proofing” our code by adding layers of abstraction to handle problems that may never arise. This results in bloated, fragile systems that are harder to maintain and extend.</p> <p>Instead, we should aim to solve the problem at hand with the <strong>simplest possible solution</strong>. Focus on clarity and readability. If the problem grows in complexity later, we can refactor. But don’t build complexity into the system before it’s necessary (wait for the last responsible moment). Simplicity forces us to focus on what’s truly important.</p> <h4 id="the-best-code-is-no-code">The best code is no code</h4> <p>One of the most effective ways to avoid complexity is to write <strong>less code</strong>. In fact, as DHH (David Heinemeier Hansson, the creator of Ruby on Rails) often promotes “best code is no code at all.” Every line of code you write is a liability: a potential source of bugs, maintenance headaches, and future complexity.</p> <p>That’s why we should always question whether new code is truly necessary. Can the problem be solved by leveraging existing libraries or frameworks? Can we use a simpler solution, like a built-in language feature or service instead of writing a custom one?</p> <p>This philosophy also aligns with the <strong>KISS</strong> (Keep It Simple, Stupid) and <strong>DRY</strong> (Don’t Repeat Yourself) principles. By avoiding unnecessary duplication and relying on well-established solutions, we can keep our codebases lean and maintainable.</p> <h4 id="embracing-pragmatism">Embracing pragmatism</h4> <p>One powerful idea that emerged particularly from <strong>Ruby on Rails</strong>, is the concept of <strong>convention over configuration</strong>. The idea is simple: by following a set of sensible defaults (conventions), you can avoid the need for endless configuration and boilerplate.</p> <p>This is pragmatism in action. Instead of reinventing the wheel every time, we lean on conventions that work well for the majority of cases (think of the 80-20 rule). This reduces the amount of code we have to write and allows us to focus on delivering business value, not wrestling with configuration and setup.</p> <p>For example, in Rails, you don’t need to specify how database columns map to object properties. Rails does it for you based on conventions (ActiveRecord over Data-Mapper/Repository pattern). This simplicity means that developers spend less time on repetitive tasks and more time solving the unique challenges of their application. Obviously there is a trade-off in regards of separation of concerns (and clean architecture) which we need to weight for with our project needs and its future growth.</p> <p>As developers, we often face the temptation to solve problems in our own unique way. But reinventing the wheel almost always leads to increased complexity and bugs. Instead, we should rely on well-established patterns and solutions whenever possible (which are most-probably battle tested in production systems already).</p> <h4 id="clarity-above-all">Clarity above all</h4> <p>One of the most important aspects of simplicity is <strong>clarity</strong>. Code that is easy to read and review is easier to maintain, extend, and debug. Code evolves, and it’s important that the people who come after us can understand and modify our code without needing to decipher a convoluted mess.</p> <p>Simple code is <strong>self-explanatory</strong>. It doesn’t rely on clever tricks, obscure syntax or comments. It follows clear conventions and avoids unnecessary complexity. When writing code, we should always prioritize <strong>clarity</strong> over cleverness. Code is read far more often than it is written, so making it easy to read and understand is crucial!</p> <blockquote class="block-tip"> <h5 id="example-1">Example</h5> <p>One of the core principles of Agile development is to deliver value incrementally and adjust as we go. By delivering in small chunks, we can get feedback early and often, making it easier to course-correct if necessary. This iterative approach keeps the codebase simple by ensuring that we’re only adding what’s absolutely necessary at each stage.</p> </blockquote> <p>At its core, simplicity is about making the <strong>pragmatic choice</strong> and focusing on delivering value, solving today’s problems, and avoiding the pitfalls of over-engineering. By embracing simplicity, we reduce accidental complexity, write less code, and focus on clarity and readability.</p> <p>I believe that simplicity isn’t just a principle - it’s a practice. And by keeping things simple, we ensure that our systems are more maintainable, more reliable, and ultimately more valuable to clients.</p>]]></content><author><name></name></author><category term="core-values"/><summary type="html"><![CDATA[The power of doing less.]]></summary></entry></feed>